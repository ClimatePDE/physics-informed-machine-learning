{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26ba465-164b-4b0a-878f-9983ce189917",
   "metadata": {},
   "source": [
    "# Probabilistic ODE solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef66db9-73a6-4e81-86f2-c904fcff7f52",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "This notebook was created to accompany a presentation of\n",
    "probabilistic ODE solvers for the seminar \"Physics-informed Machine Learning\" at the University of Tübingen (winter term 2024/2025).\n",
    "\n",
    "It intends to explain the concept of probabilistic ODE solvers (ODE filters and smoothers) and will guide through a simple implementation of a probabilistic ODE solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cff49c-8668-4f36-b78a-667ae1a09bf1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial is based on:\n",
    "\n",
    "- Mostly, esp. the theoretical parts:\n",
    "  -  Philipp Hennig, Michael A. Osborne, and Hans P. Kersting\n",
    "    - Probabilistic Numerics: Computation as Machine Learning. Cambridge University Press, 2022\n",
    "    - DOI [10.1017/9781316681411](https://doi.org/10.1017/9781316681411)\n",
    "    - Also available for free personal use here: [https://www.probabilistic-numerics.org/textbooks/](https://www.probabilistic-numerics.org/textbooks/)\n",
    "    - I tried to keep the notation close to theirs\n",
    "- And also in parts:\n",
    "  - Filip Tronarp et al.\n",
    "    - Probabilistic Solutions to Ordinary Differential Equations as Nonlinear Bayesian Filtering: A New Perspective\n",
    "    - In: Statistics and Computing 29.6 (Nov. 2019), pp. 1297–1315. issn: 1573-1375.\n",
    "    - DOI [10.1007/s11222-019-09900-1](https://doi.org/10.1007/s11222-019-09900-1)\n",
    "- The latter parts on numerical stability and the implementation are based on:\n",
    "  - Nicholas Krämer, Philipp Hennig\n",
    "  - Stable Implementation of Probabilistic ODE Solvers\n",
    "  - On ArXiv, Dec. 2020\n",
    "  - DOI [10.48550/arXiv.2012.10106](https://doi.org/10.48550/arXiv.2012.10106)\n",
    "\n",
    "Where other sources are used, a reference/link is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25021b-c197-4251-8d7d-1207d3baea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies here (uncomment first)\n",
    "# cuda\n",
    "#!pip install numpy scipy matplotlib 'jax[cuda]' diffrax\n",
    "\n",
    "# no cuda\n",
    "#!pip install numpy scipy matplotlib jax diffrax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3255b2-d066-490e-8c94-f0a0483c52b9",
   "metadata": {},
   "source": [
    "JAX preallocates 75% of GPU memory.\n",
    "\n",
    "If you run into OOM errors at the start,\n",
    "you can try to circumvent this by running this process\n",
    "(ie. the jupyter server or whatever you use to run the notebook)\n",
    "with one of these environment variables:\n",
    "\n",
    "- XLA_PYTHON_CLIENT_MEM_FRACTION=.50 (% of preallocated mem; or set it even lower); or:\n",
    "- XLA_PYTHON_CLIENT_PREALLOCATE=false (disable preallocation entirely)\n",
    "\n",
    "See also: [JAX GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654733-dffe-4d6c-ab36-3aa8d4b5dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "import jax\n",
    "import scipy\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.experimental import jet\n",
    "from diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n",
    "# Use float64 precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "# higher plot resolution\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59d0a2-4188-4ca7-9b5e-afe079287626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for later\n",
    "def plot_lv_with_uncertainty(prob_sol, num_stddevs=2, c_sol=None, title=None):\n",
    "    predator_col = \"red\"\n",
    "    prey_col = \"green\"\n",
    "    if c_sol is not None:\n",
    "        plt.plot(sol.ts, sol.ys, label=None, linewidth=1, color=\"grey\")\n",
    "    plt.plot(prob_sol[\"ts\"], prob_sol[\"ys\"][:, 0], label=\"Prey\",\n",
    "             color=prey_col, linewidth=1)\n",
    "    plt.plot(prob_sol[\"ts\"], prob_sol[\"ys\"][:, 1], label=\"Predator\",\n",
    "             color=predator_col, linewidth=1)\n",
    "    plt.fill_between(prob_sol[\"ts\"],\n",
    "                     prob_sol[\"ys\"][:, 0] - num_stddevs*prob_sol[\"stddevs\"][:, 0],\n",
    "                     prob_sol[\"ys\"][:, 0] + num_stddevs*prob_sol[\"stddevs\"][:, 0],\n",
    "                     label=f\"Prey $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=prey_col, alpha=0.4)\n",
    "    plt.fill_between(prob_sol[\"ts\"],\n",
    "                     prob_sol[\"ys\"][:, 1] - num_stddevs*prob_sol[\"stddevs\"][:, 1],\n",
    "                     prob_sol[\"ys\"][:, 1] + num_stddevs*prob_sol[\"stddevs\"][:, 1],\n",
    "                     label=f\"Predator $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=predator_col, alpha=0.4)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend(prop={'size': 6})\n",
    "    plt.show()\n",
    "\n",
    "def plot_filter_and_smoother(f_out, s_out, num_stddevs=2, c_sol=None, title=None):\n",
    "    predator_col = \"red\"\n",
    "    prey_col = \"green\"\n",
    "    filter_col = \"grey\"\n",
    "    if c_sol is not None:\n",
    "        plt.plot(sol.ts, sol.ys, label=None, linewidth=1, color=\"black\")\n",
    "    plt.plot(f_out[\"ts\"], f_out[\"ys\"][:, 0], label=\"Prey (Filter)\",\n",
    "             color=filter_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(f_out[\"ts\"], f_out[\"ys\"][:, 1], label=\"Predator (Filter)\",\n",
    "             color=filter_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(s_out[\"ts\"], s_out[\"ys\"][:, 0], label=\"Prey (Smoother)\",\n",
    "             color=prey_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(s_out[\"ts\"], s_out[\"ys\"][:, 1], label=\"Predator (Smoother)\",\n",
    "             color=predator_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.fill_between(f_out[\"ts\"],\n",
    "                     f_out[\"ys\"][:, 0] - num_stddevs*f_out[\"stddevs\"][:, 0],\n",
    "                     f_out[\"ys\"][:, 0] + num_stddevs*f_out[\"stddevs\"][:, 0],\n",
    "                     #label=f\"Prey (Filter) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     label=None,\n",
    "                     linewidth=0, color=filter_col, alpha=0.3)\n",
    "    plt.fill_between(f_out[\"ts\"],\n",
    "                     f_out[\"ys\"][:, 1] - num_stddevs*f_out[\"stddevs\"][:, 1],\n",
    "                     f_out[\"ys\"][:, 1] + num_stddevs*f_out[\"stddevs\"][:, 1],\n",
    "                     #label=f\"Predator (Filter) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     label=None,\n",
    "                     linewidth=0, color=filter_col, alpha=0.3)\n",
    "    plt.fill_between(s_out[\"ts\"],\n",
    "                     s_out[\"ys\"][:, 0] - num_stddevs*s_out[\"stddevs\"][:, 0],\n",
    "                     s_out[\"ys\"][:, 0] + num_stddevs*s_out[\"stddevs\"][:, 0],\n",
    "                     label=f\"Prey (Smoother) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=prey_col, alpha=0.4)\n",
    "    plt.fill_between(s_out[\"ts\"],\n",
    "                     s_out[\"ys\"][:, 1] - num_stddevs*s_out[\"stddevs\"][:, 1],\n",
    "                     s_out[\"ys\"][:, 1] + num_stddevs*s_out[\"stddevs\"][:, 1],\n",
    "                     label=f\"Predator (Smoother) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=predator_col, alpha=0.4)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend(prop={'size': 6})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885f2e1-3995-4d4c-abfd-46c420108861",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3619ae3-db90-4a24-991c-fb77a628305d",
   "metadata": {},
   "source": [
    "### Ordinary Differential Equations (ODEs)\n",
    "\n",
    "_Here_, an ODE is a function\n",
    "$$ y'(t) = f(y(t)) \\in \\mathbb{R}^d\\,, t \\in [0, T]\\,, $$\n",
    "and we want to solve it by finding a curve\n",
    "$$ y: [0, T] \\rightarrow \\mathbb{R}^d $$\n",
    "with initial value\n",
    "$$ y(0) = y_0 $$\n",
    "that satisfies this equation.\n",
    "\n",
    "This means that, as opposed to general ODEs, we require,\n",
    "mainly to keep the theoretical parts simple:\n",
    "\n",
    "- The ODE only contains the first derivative $y'(t)$, no higher-order derivatives\n",
    "  - ie. we only consider _first-order_ ODEs\n",
    "  - This comes without loss of generality:\n",
    "    - all higher-order ODEs can be rewritten as first-order ODE\n",
    "    - But: the solver _could_ be extended to also consider higher-order ODE information\n",
    "- $f$ does not depend on $t$ ($\\rightarrow$ autonomous ODE)\n",
    "  - Also no loss of generality\n",
    "    - Again, every ODE can be transformed into autonomous form\n",
    "- The equation given above is in _explicit_ form\n",
    "  - we won't consider _implicit_ ODEs (ie. of form $ f(y(t), y'(t)) = 0 $)\n",
    "- A known initial value, so we are actually interested in an ODE _initial value problem_ (IVP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e5e38-b2ea-46c7-a2ff-12486c31cde0",
   "metadata": {},
   "source": [
    "#### Lotka-Volterra system\n",
    "\n",
    "The Lotka-Volterra system/equations model predator/prey-populations as a 2-dimensional ODE:\n",
    "\\begin{align*}\n",
    "    y_1'(t) &= \\alpha y_1(t) - \\beta y_1(t) y_2(t) \\\\\n",
    "    y_2'(t) &= -\\gamma y_2(t) + \\delta y_1(t) y_2(t)\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $y_1$ is the population density/size of the prey\n",
    "- $y_2$ is the population density of the predator\n",
    "  - $\\alpha$: prey growth rate\n",
    "  - $\\gamma$: predator death rate\n",
    "  - $\\beta$: effect of predator population density on prey death rate\n",
    "  - $\\delta$: effect of prey population density on predator growth rate\n",
    "\n",
    "For more details, see eg. [Lotka–Volterra equations (Wikipedia)](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations).\n",
    "\n",
    "Let's see an example of it, using a classical ODE solver\n",
    "(code taken from [Diffrax documentation](https://docs.kidger.site/diffrax/examples/coupled_odes/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda6fa6-e6b2-4e46-9554-78502e2b0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"args\"])\n",
    "def lotka_volterra_vf(t, y, args):\n",
    "    prey, predator = y[..., 0], y[..., 1]\n",
    "    alpha, beta, gamma, delta = args\n",
    "    d_prey: Array = alpha * prey - beta * prey * predator\n",
    "    d_predator: Array = -gamma * predator + delta * prey * predator\n",
    "    return jnp.array([d_prey, d_predator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5456d5-37df-45fd-bc37-d1ba7d0cde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0\n",
    "t1 = 140\n",
    "y0 = jnp.array([10.0, 10.0])\n",
    "lv_args = (0.1, 0.02, 0.4, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64416d46-903e-4aff-b398-02136f674f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = ODETerm(lotka_volterra_vf)\n",
    "solver = Tsit5()\n",
    "saveat = SaveAt(ts=jnp.linspace(t0, t1, 1000))\n",
    "dt0 = 0.1\n",
    "sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=lv_args, saveat=saveat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf08519-048c-4656-88e0-2c7ca763afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sol.ts, sol.ys, label=[\"Prey\", \"Predator\"])\n",
    "plt.legend()\n",
    "plt.title(\"Lotka-Volterra system (Tsit5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bcec2-2421-44ef-a85a-70e9e7e73ec0",
   "metadata": {},
   "source": [
    "### Classical ODE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cf4b2-d296-460d-aae4-1ded22b0f4ee",
   "metadata": {},
   "source": [
    "#### How do they work?\n",
    "\n",
    "- Work on discrete time steps:\n",
    "  $$ 0 = t_0 < t_1 < \\ldots < t_N = T\\,, \\text{ with step sizes: } h_n = t_n - t_{n - 1} $$\n",
    "- Iteratively perform steps:\n",
    "  - At step $n$, we have $\\hat{y}(t_{n-1}) \\approx y(t_{n-1})$\n",
    "  - Estimate derivative $y'(t_{n-1}) = f(y(t_{n-1})) \\approx f(\\hat{y}(t_{n-1}))$\n",
    "    - Potentially use more information\n",
    "    - Eg. approximate higher-order derivatives\n",
    "  - Construct approximation to $\\hat{y}(t_n) = \\hat{y}(t_{n-1} + h_n)$\n",
    "    - Using derivative information/other information\n",
    "    - Example: Eulers method\n",
    "      - $\\hat{y}(t_{n-1} + h_n) \\approx \\hat{y}(t_{n-1}) + h_n f(\\hat{y}(t_{n-1}))$\n",
    "      - First-order Taylor expansion (linear approximation) of $\\hat{y}(t_{n-1} + h_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c8e18-b18b-4a34-b85b-ec3cff4a6893",
   "metadata": {},
   "source": [
    "#### Order\n",
    "\n",
    "An ODE solver of order $q$\n",
    "\n",
    "- essentially fits a $q$-th order Taylor expansion\n",
    "- thus has _local_ error (or convergence rate) $\\mathcal{O}(h^{q+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647267f-4678-4397-9cc6-6cbe05f29b1c",
   "metadata": {},
   "source": [
    "### Probabilistic viewpoint: Viewing ODE solving as regression\n",
    "\n",
    "One can view solving an ODE as regression on\n",
    "$$ \\mathcal{D} = \\left\\{x(0) = x_0, x'(t_n) = f(\\hat{x}(t_n)) \\mid\n",
    "n \\in \\{0, \\ldots, N\\}\\right\\} $$\n",
    "(that is, all the data available to the solver),\n",
    "where the goal is to find a function $x$ that fulfills all of these equations,\n",
    "and the data is revealed to the solver progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306daf5-d841-4a66-8c45-62b958127b3e",
   "metadata": {},
   "source": [
    "## Probabilistic ODE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdaf9c-2213-4da4-b3d9-33b61a176150",
   "metadata": {},
   "source": [
    "### Why?\n",
    "\n",
    "- Classical solvers treat the current estimate $\\hat{y}(t_n) \\approx y(t_n)$\n",
    "  as the true value when taking a step, without taking into account eg.\n",
    "  accumulated numerical errors\n",
    "  - Probabilistic solvers provide a very natural way of expressing uncertainty\n",
    "    - Not only do you get an estimate of uncertainty in the solution, but:\n",
    "    - The solver can make use of that information while solving the ODE\n",
    "- They also allow to model uncertainty in more than just the solution, for example\n",
    "  - Uncertainty in the initial value $y_0$\n",
    "  - Or some uncertainty in the ODE itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961541a9-57fe-4237-8fec-5d26dbc6b7b3",
   "metadata": {},
   "source": [
    "### GP Regression\n",
    "\n",
    "Jointly model $y(t)$ and $y'(t)$ in a state vector:\n",
    "$$ \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix} = x(t) \\sim \\mathcal{GP}(\\bar{x}, k) $$\n",
    "with mean function $\\bar{x}(t)$ and kernel function $k(t_1, t_2)$.\n",
    "\n",
    "How to perform inference efficiently?\n",
    "- In complexity $\\mathcal{O}(N)$\n",
    "  - instead of $\\mathcal{O}(N^3)$ for traditional GP regression\n",
    "- How to take advantage of the derivative information?\n",
    "\n",
    "$\\rightarrow$ we need a sensible prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d3593-3a4e-436f-a3bb-e46fd490c61f",
   "metadata": {},
   "source": [
    "### Model ODE solution in Continuous-Time State Space Model (SSM)\n",
    "\n",
    "We can model the state vector $x(t) \\in \\mathbb{R}^{q}$ by a stochastic process:\n",
    "$$ x(t) \\sim X(t) $$\n",
    "and introduce projections $H_0, H \\in \\mathbb{R}^{d \\times (q+1)}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    y(t) &= H_0 x(t) \\\\\n",
    "    y'(t) &= H x(t) \\,.\n",
    "\\end{align*}\n",
    "\n",
    "Since we are interested in IVPs, we also model the initial value by\n",
    "$$ X(0) \\sim \\mathcal{N}(m_0, P_0) $$\n",
    "(more details later on).\n",
    "\n",
    "An example where $x$ models $y(t)$ and its first three derivatives (ie. $q = 3$):\n",
    "\n",
    "\\begin{align*}\n",
    "    H_0 &= \\begin{bmatrix} 1 & 0 & 0 & 0 \\end{bmatrix} \\\\\n",
    "    H &= \\begin{bmatrix} 0 & 1 & 0 & 0 \\end{bmatrix} \\\\\n",
    "    x(t) &= \\begin{pmatrix} y(t) \\\\ y'(t) \\\\ y''(t) \\\\ y'''(t) \\\\ \\end{pmatrix} \\sim X(t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937f9a1-3225-4309-9412-9fd222762720",
   "metadata": {},
   "source": [
    "We can build the projection matrices as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897014d-e38d-48a4-81ae-46f4fec93026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssm_projection_matrices(d, q):\n",
    "    # Projections: project SSM state -> y (H0) or SSM state -> y' (H)\n",
    "    H0 = jnp.zeros((1, q + 1)).at[0, 0].set(1.0)\n",
    "    H = jnp.zeros((1, q + 1)).at[0, 1].set(1.0)\n",
    "    Id = jnp.eye(d)  # Ignore this and below for now.\n",
    "    H0 = jnp.kron(H0, Id)\n",
    "    H = jnp.kron(H, Id)\n",
    "    return H0, H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627be9c3-f3dd-4b13-86fc-b9def4763e6f",
   "metadata": {},
   "source": [
    "#### Dynamic model / State\n",
    "\n",
    "We now let $X(t)$ follow this linear, time-invariant SDE:\n",
    "$$ dx(t) = Fx(t)dt + Ld\\omega_t $$\n",
    "where $ F = \\begin{bmatrix} 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\\\ 0&0&0&0 \\end{bmatrix} $\n",
    "and $ L = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\sigma \\end{pmatrix} $, and $d\\omega_t$\n",
    "can be informally thought of as \"increment of the Wiener process\";\n",
    "integrating over it yields the Wiener process, ie. the random walk.\n",
    "\n",
    "This gives rise to an interesting structure:\n",
    "\n",
    "- all elements in $x(t)$ are derivatives of each other for all $t$\n",
    "- $y'''(t)$ is modeled as random walk with scale $\\sigma$\n",
    "  (as its derivative is the increment of the Wiener process)\n",
    "\n",
    "Because:\n",
    "$$\n",
    "dx(t)\n",
    "= \\begin{bmatrix} 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\\\ 0&0&0&0 \\end{bmatrix}\n",
    "\\begin{pmatrix} y(t) \\\\ y'(t) \\\\ y''(t) \\\\ y'''(t) \\\\ \\end{pmatrix} dt\n",
    "+ \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\sigma \\end{pmatrix} d\\omega_t\n",
    "= \\begin{pmatrix} y'(t)dt \\\\ y''(t)dt \\\\ y'''(t)dt \\\\ d\\omega_t \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This means that $x(t)$ now models some function $y(t)$ and its $q=3$ first\n",
    "derivatives, and $y'''(t)$ is modeled as Wiener process/random walk.\n",
    "\n",
    "Thus, this Prior is known as the $q$-times integrated Wiener process (with scale $\\sigma$).\n",
    "\n",
    "Also, as is evident above, the $q$-times IWP prior is a GP that satisfies the Markov property\n",
    "(referred to as Gauss-Markov process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1914d9-4f3f-446d-bdb9-9511b975d573",
   "metadata": {},
   "source": [
    "#### Measurement model (Information operator)\n",
    "\n",
    "To our dynamic model, we now add a measurement model.\n",
    "\n",
    "We define:\n",
    "$$ z(t) \\sim Z(t) := g(X(t)) := HX(t) - f(H_0 X(t))\\,, $$\n",
    "so\n",
    "$$ z(t) = g(x(t)) = Hx(t) - f(H_0 x(t)) = y'(t) - f(y(t))\\,, $$\n",
    "which is referred to as the _state misalignment_.\n",
    "\n",
    "$Z$ now serves as _information operator_:\n",
    "It extracts information about the the state misalignment $z(t)$ from the ODE,\n",
    "quantifying the \"correctness\" of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c968fa-4ebf-42b0-82f9-f9240fbd62c5",
   "metadata": {},
   "source": [
    "#### Likelihood\n",
    "\n",
    "Now, we have our prior (the $q$-times IWP), and a measurement model (through the information operator).\n",
    "\n",
    "As seen above, $z(t) = y'(t) - f(y(t))$, which means that iff $y$ is a solution to the ODE (ie. $y'(t) = f(y(t))$ for all $t$; and of course also $y(0) = y_0$),\n",
    "then $z(t) = 0$.\n",
    "\n",
    "Thus, if we use the likelihood\n",
    "$$ p(z(t) \\mid x(t)) = \\delta(g(x(t))) = \\delta(Hx(t) - f(H_0 x(t))) = \\delta(y'(t) - f(y(t)))\\,, $$\n",
    "then, if we simply observe the data $0 = z(t)$ for all $t$,\n",
    "the posterior $p(x \\mid z = 0)$ contains the solution (also with zero uncertainty).\n",
    "\n",
    "Now, we have modeled the ODE (solution) as a _filtering problem_,\n",
    "and inference in this setting and observations $z(t) = 0$ does\n",
    "recover the ODE solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee2e24-f9c1-4d43-9b66-4f13c5724146",
   "metadata": {},
   "source": [
    "### Discrete-time version of the SSM\n",
    "\n",
    "Of course, inference in this model is intractable,\n",
    "we can only work on discrete (and finitely many) points $t_n$;\n",
    "for tractable inference, we need to discretize the above continuous-time SSM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599bbf2-cce4-4ce4-adf7-9b1c7b445323",
   "metadata": {},
   "source": [
    "With functions of $t_n$ now denoted by subscript $n$ (eg. $x(t_n) := x_n$),\n",
    "the above can be discretized into\n",
    "\\begin{align}\n",
    "    p(x_0) &= \\mathcal{N}(x_0; m_0, P_0) &\\text{Initial value}\\\\\n",
    "    p(x_{n+1} \\mid x_n) &= \\mathcal{N}(x_{n+1}; A(h_{n+1})x_n, Q(h_{n+1})) &\\text{Dynamics}\\\\\n",
    "    p(z_n \\mid x_n) &= \\delta(z_n - g(x_n)) &\\text{Observations (information operator)}\n",
    "\\end{align}\n",
    "\n",
    "Or, alternatively,\n",
    "\\begin{align}\n",
    "    p(z_n \\mid x_n) &= \\mathcal{N}(z_n; g(x_n), R) &\\text{Observations (information operator)}\\,,\n",
    "\\end{align}\n",
    "which is less general (but the variance $R$ can help in some cases),\n",
    "and, if the vector field $f$ is linear, this is a linear Gaussian SSM (as $g$ then is linear too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb6c6f-dfc6-4684-b943-4a7a43f4af59",
   "metadata": {},
   "source": [
    "In the discretized SSM, we have $A(h_n), Q(h_n)$ given by\n",
    "\\begin{align*}\n",
    "    A(h_n)_{i,j} &= \\mathbb{I}(j \\geq i) \\frac{h_n^{j-i}}{(j-i)!} &\\text{Transition matrix}\\\\\n",
    "    Q(h_n)_{i,j} &= \\sigma^2 \\frac{h_n^{2q + 3 - i - j}}{(2q + 3 - i - j)(q + 1 - i)!(q + 1 - j)!} &\\text{Diffusion matrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76b226-f229-44ab-be00-d407427af899",
   "metadata": {},
   "source": [
    "What does this mean?\n",
    "\n",
    "Consider again the example from above:\n",
    "$$ x_n = \\begin{pmatrix} y_n \\\\ y'_n \\\\ y''_n \\\\ y'''_n \\end{pmatrix}\\,, $$\n",
    "then, with $h := h_{n+1}, A := A(h_{n+1})$ for less clutter:\n",
    "$$ (Ax_n)_1 =\n",
    "\\begin{bmatrix}\\frac{h^0}{0!} & \\frac{h^1}{1!} & \\frac{h^2}{2!} & \\frac{h^3}{3!}\\end{bmatrix}\n",
    "\\begin{pmatrix} y_n \\\\ y'_n \\\\ y''_n \\\\ y'''_n \\end{pmatrix}\n",
    "= y_n + h y'_n + \\frac{h^2}{2} y''_n + \\frac{h^3}{6} y'''_n\\,, $$\n",
    "which is a $q=3$-rd order Tayler expansion of $y(t_n + h_{n+1})$.\n",
    "\n",
    "- Thus, we'll also get a solver of order $q$ (and local error $\\mathcal{O}(h^{q+1})$)\n",
    "  - This is _just for illustration_, absolutely not a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff35a0-e483-4071-bc0d-02b62d60df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_transition_matrix(d, q, h):\n",
    "    A_exponents = jnp.stack([jnp.arange(q + 1) - i for i in range(q + 1)])\n",
    "    A = jnp.triu(h ** A_exponents / jax.scipy.special.factorial(A_exponents))\n",
    "    return jnp.kron(A, jnp.eye(d))  # Ignore everything about d/this line for now\n",
    "\n",
    "def discrete_diffusion_matrix(d, q, h):\n",
    "    Q_j = jnp.stack((jnp.arange(q + 1) + 1,) * (q + 1))\n",
    "    Q_exponent = 2 * q + 3 - Q_j.T - Q_j\n",
    "    Q_divisor_j = jax.scipy.special.factorial(q + 1 - Q_j)\n",
    "    Q = (h ** Q_exponent  # Note the absence of the \\sigma^2 factor,\n",
    "         / (Q_exponent    # we'll come back to this\n",
    "             * Q_divisor_j\n",
    "             * Q_divisor_j.T))\n",
    "    return jnp.kron(Q, jnp.eye(d))  # Ignore everything about d/this line for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ab4a5-2761-41b7-8d1f-3352b0d6661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example with q=2 and h=0.1:\n",
    "A_ex = discrete_transition_matrix(1, 2, 0.1)\n",
    "Q_ex = discrete_diffusion_matrix(1, 2, 0.1)\n",
    "print(f\"A:\\n{A_ex}\")\n",
    "print(f\"Q:\\n{Q_ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8cb08-335b-43ba-b4be-6999e9f67593",
   "metadata": {},
   "source": [
    "#### Why use state misalignment z and not condition on $f(H_0 x_n)$ directly?\n",
    "\n",
    "- This separates the observations from the implementation/algorithm, ie. we observe the data $z_n = 0$ for _any_ implementation\n",
    "  - If we changed the observations to:\n",
    "    - $\\tilde{z}_n = Hx_n$\n",
    "    - and want to observe that $\\tilde{z}_n = f(H_0 x_n)$ directly\n",
    "  - then the observations/data that we condition on depend on the algorithm\n",
    "    - as $x_n$, the current solution/state, depends on the algorithm in use\n",
    "    - so every algorithm would operate on different observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9540027-57ed-402c-be27-341f3b444bcc",
   "metadata": {},
   "source": [
    "## Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261b361-fb32-44f3-86e2-1a79026794ba",
   "metadata": {},
   "source": [
    "### Kalman (ODE) Filter\n",
    "\n",
    "For now, let's first consider the linear Gaussian SSM case from above:\n",
    "\\begin{align}\n",
    "    p(x_0) &= \\mathcal{N}(x_0; m_0, P_0) &\\text{Initial value}\\\\\n",
    "    p(x_{n+1} \\mid x_n) &= \\mathcal{N}(x_{n+1}; A(h_{n+1})x_n, Q(h_{n+1})) &\\text{Dynamics}\\\\\n",
    "    p(z_n \\mid x_n) &= \\mathcal{N}(z_n; g(x_n), R) &\\text{Observations (information operator)}\\end{align}\n",
    "\n",
    "On this, we can directly perform Bayesian inference.\n",
    "As the SSM is Markovian, we can perform filtering by iterating two steps:\n",
    "\n",
    "- Predict: $p(x_{n+1} | z_{0:n}) = \\int p(x_{n+1} | x_n)p(x_n | z_{0:n}) dx_n$ (Chapman-Kolmogorov Eq.)\n",
    "- Observe the next $z_{n+1}$\n",
    "  - Actually: observe that $z_{n+1} = 0$\n",
    "  - So we don't really need to do anything\n",
    "- Update: $p(x_{n+1} | z_{0:n+1}) = \\frac{p(z_{n+1} | x_{n+1})p(x_{n+1} | z_{0:n})}{p(z_{n+1}|z_{0:n})}$\n",
    "\n",
    "As (for now) everything is linear Gaussian, the above just boils down\n",
    "to a little bit of linear algebra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe28074-6f4b-4c1f-b321-f88fa5999c78",
   "metadata": {},
   "source": [
    "#### Prediction Step\n",
    "\n",
    "$$ p(x_{n+1} | z_{0:n}) = \\mathcal{N}(x_{n+1}; m_{n+1}^p, P_{n+1}^p) $$\n",
    "where\n",
    "\\begin{align*}\n",
    "    m_{n+1}^p &= A(h_{n+1}) m_n^f &\\text{predictive mean}\\\\\n",
    "    P_{n+1}^p &= A(h_{n+1}) P_n^f A(h_{n+1})^\\top + Q(h_{n+1}) &\\text{predictive cov}\\,,\n",
    "\\end{align*}\n",
    "and this is simply implemented by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bd205-69f1-48f7-b2a1-6020b0a19c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(m_f, P_f, A, Q):\n",
    "    m_p = A @ m_f  # predictive mean\n",
    "    P_p = A @ P_f @ A.T + Q  # predictive cov\n",
    "    return m_p, P_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721e08a-54d8-4e61-8114-88865f9c3802",
   "metadata": {},
   "source": [
    "#### Exact Update Step ($f$ linear)\n",
    "\n",
    "$$ p(x_{n+1} | z_{0:n+1}) = \\mathcal{N}(x_{n+1}; m_{n+1}^f, P_{n+1}^f) $$\n",
    "where\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= H &\\text{(for now)} \\\\\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov} \\,.\n",
    "\\end{align*}\n",
    "\n",
    "As long as $f$ is linear, everything stays linear Gaussian.\n",
    "- This is known as the _Kalman filter_\n",
    "- And is exact\n",
    "\n",
    "Before we implement this: What can we do for nonlinear $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4ca82-73c8-4392-840e-242fd8d39f94",
   "metadata": {},
   "source": [
    "#### Extended Kalman ODE Filter (EKF0, EKF1)\n",
    "\n",
    "We need to stay linear Gaussian, but our $f$ now is nonlinear :(\n",
    "\n",
    "What to do now?\n",
    "- We can just approximate $f$ around $H_0 m_{n+1}^p$ by a linear function, then do the above\n",
    "- Thus, the update is now no longer exact, but:\n",
    "  - This is very cheap\n",
    "  - and often accurate enough\n",
    "\n",
    "This is known as Extended Kalman (ODE) Filter (EKF).\n",
    "\n",
    "We have two choices for our approximation of $f$:\n",
    "- constant function (order 0 Taylor expansion)\n",
    "  - EKF0\n",
    "  - $f(H_0 m_{n+1}^p + y) \\approx f(H_0 m_{n+1}^p)$\n",
    "- linear function (order 1 Taylor expansion)\n",
    "  - EKF1\n",
    "  - $f(H_0 m_{n+1}^p + y) \\approx f(H_0 m_{n+1}^p) + J_f(H_0 m_{n+1}^p)(y - H_0 m_{n+1}^p)$\n",
    "\n",
    "From this, we obtain two choices for the EKF update step:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= \\begin{cases}H &\\text{EKF0} \\\\ H - J_f(H_0 m_{n+1}^p) H_0 &\\text{EKF1}\\end{cases} &\\text{cov. of $z_{n+1}$ after approximation}\\\\\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov} \\,.\n",
    "\\end{align*}\n",
    "\n",
    "With that, we can directly implement the update:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b75e7-a608-4f9b-bb23-1d542d276d7a",
   "metadata": {},
   "source": [
    "Calculate:\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= \\begin{cases}H &\\text{EKF0} \\\\ H - J_f(H_0 m_{n+1}^p) H_0 &\\text{EKF1}\\end{cases} &\\text{cov. of $z_{n+1}$ after approximation}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b8479-c425-435f-bb58-894906fbeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(f, t, m_p, H, H0, f_args, ekf_order):\n",
    "    # returns the innovation residual/state misalignment\n",
    "    m_p0 = H0 @ m_p\n",
    "    f_at_m_p = f(t, m_p0, f_args)\n",
    "    if ekf_order == 0:\n",
    "        H_hat = H\n",
    "    else:\n",
    "        # ekf_order == 1\n",
    "        # Calculate Jacobian of f with forward mode AD\n",
    "        J_f = jax.jacfwd(f, argnums=1)(t, m_p0, f_args)\n",
    "        H_hat = H - J_f @ H0  # regular * in 1-d case\n",
    "    z_hat = f_at_m_p - H @ m_p  # residual\n",
    "    return z_hat, H_hat  # H_hat: \\tilde{H} from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434af55b-ec08-44b0-b1ef-d0683b1b25fd",
   "metadata": {},
   "source": [
    "Calculate:\n",
    "\\begin{align*}\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov (intermediate)} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain (intermediate)} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean (result)} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov (result)} \\,.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d91b6-c738-4337-8f75-2da392928d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R):\n",
    "    # Kalman filter statistics\n",
    "    S = H_hat @ P_p @ H_hat.T + R\n",
    "    S_inv = jnp.linalg.inv(S)  # Yes, inverse here. We'll improve this later.\n",
    "    K = P_p @ H_hat.T @ S_inv\n",
    "    # Filtering mean/cov\n",
    "    m_f = m_p + K @ z_hat\n",
    "    P_f = (jnp.eye(P_p.shape[1]) - K @ H_hat) @ P_p\n",
    "    return m_f, P_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69b5d3-f317-4f14-b50f-f39ad703ac73",
   "metadata": {},
   "source": [
    "#### Filter step\n",
    "\n",
    "Thus, we can build a full filter step (predict, then update) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0bae1-576e-49c7-b15c-4e6a0678ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jit compile this function for speedup\n",
    "@partial(jax.jit, static_argnames=[\"f\", \"f_args\", \"d\", \"q\", \"ekf_order\"])\n",
    "def ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                    t_prev, h, d, q,\n",
    "                    H0, H, R, ekf_order):\n",
    "    # calculate next time step\n",
    "    t = t_prev + h\n",
    "    # Get discretization for stepsize h:\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    Q = discrete_diffusion_matrix(d, q, h)\n",
    "    # predict\n",
    "    m_p, P_p = predict(m_f_prev, P_f_prev, A, Q)\n",
    "    # update\n",
    "    z_hat, H_hat = residual(f, t, m_p, H, H0, f_args, ekf_order)\n",
    "    m_f, P_f = next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R)\n",
    "    # Also extract and return the current (filtering) y and its stddev\n",
    "    y = H0 @ m_f\n",
    "    stddev = jnp.sqrt(jnp.diag(H0 @ P_f @ H0.T))\n",
    "    return m_f, P_f, m_p, P_p, y, stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc1bbd-7f7b-4912-9a18-00f62d285a0b",
   "metadata": {},
   "source": [
    "#### How to get the initial state vector?\n",
    "\n",
    "Remember, the state vector $x_n$ contains $y_n$ and $y'_n$,\n",
    "and, if we want, derivatives up to $y^{(q)}_n$.\n",
    "\n",
    "If $q = 1$, then it is obvious that $x_0 = \\begin{pmatrix} y_0 & f(y_0) \\end{pmatrix}^\\top$,\n",
    "but how do we handle higher $q$?\n",
    "- The elements of $x$ are, and stay, derivatives of each other, so we need to initialize\n",
    "  the first mean with higher-order derivatives as follows:\n",
    "  $$ m_0^f = \\begin{pmatrix} y_0 & f^{\\langle 1 \\rangle}(y_0) & f^{\\langle 2 \\rangle}(y_0) & \\ldots & f^{\\langle q \\rangle}(y_0)\\end{pmatrix}^\\top\\,, $$\n",
    "  where\n",
    "  \\begin{align*}\n",
    "    f^{\\langle 1 \\rangle}(y) &= f(y)\\,, \\\\\n",
    "    f^{\\langle i \\rangle}(y) &= \\frac{d}{dy} \\left( f^{\\langle i-1 \\rangle}(y) \\cdot f(y) \\right) & \\text{(for $i>1$)}\\,.\n",
    "  \\end{align*}\n",
    "  This is exponentially (in $q$) expensive with regular AD,\n",
    "  but can be more efficiently calculated using Taylor-mode AD, in JAX implemented in ```jet.jet()```.\n",
    "\n",
    "\n",
    "If interested, more details can be found in [Bettencourt, Johnson, Duvenaud (2019)](https://openreview.net/pdf?id=SkxEF3FNPH]).\n",
    "\n",
    "- This is the only thing we need to worry about (apart from stability issues, maybe addressed at the end)\n",
    "\n",
    "Here's an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e075e-e982-428b-b479-1bb73f4364e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state(f, t0, y0, q, f_args):\n",
    "    # Return [y0, f^<i>(y0)] as defined above\n",
    "    def f_wrapped(y):\n",
    "        # make f a function of y only (so it works with jet)\n",
    "        return f(t0, y, f_args)\n",
    "    y0_tup = (y0,)  # jet wants this in a tuple\n",
    "    # y_is will now iteratively be filled like this:\n",
    "    # (f(y0), f^<1>(y0), f^<2>(y0))  ->  (f(y0),) + (f^<1>(y0), f^<2>(y0), f^<3>(y0))\n",
    "    y_is = (f_wrapped(y0),)\n",
    "    for _ in range(q - 1):\n",
    "        y_i_new, y_is_new = jet.jet(f_wrapped, y0_tup, (y_is,))\n",
    "        # above returns tuple[Array, list[Array]], put all into one tuple\n",
    "        y_is = (y_i_new,) + (*y_is_new,)\n",
    "    return jnp.stack((y0_tup) + y_is).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c746d-0f3b-4268-8685-5f2b632607cf",
   "metadata": {},
   "source": [
    "#### How to fit higher-dimensional y? (ie. an ODE system)\n",
    "\n",
    "If $y$ is $d$-dimensional, then we can just stack everything, using the Kronecker product $\\otimes$,\n",
    "with\n",
    "$$\n",
    "    \\mathbf{A} = A \\otimes I_d\n",
    "       = \\begin{bmatrix}\n",
    "          a_{1,1}I_d & \\cdots & a_{1,q+1}I_d \\\\\n",
    "          \\vdots & \\ddots & \\vdots \\\\\n",
    "          a_{q+1,1}I_d & \\cdots & a_{q+1,q+1}I_d\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{d(q+1)\\times d(q+1)}\\,.\n",
    "$$\n",
    "For example, for $d=2, q=1$, we have\n",
    "$$ \\mathbf{x} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_1' \\\\ y_2' \\end{pmatrix}\\,, $$\n",
    "and $\\mathbf{A} = A \\otimes I_2$.\n",
    "\n",
    "The functions given above for $A, Q, H, H_0$ already do this, so let's see what this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5f677-c505-46cb-95ee-b1f7b875cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = jnp.array([0.0, 2.0])  # the first dimension\n",
    "x2 = jnp.array([1.0, 3.0])  # the second dimension\n",
    "x_s = jnp.stack([x1, x2]).T.flatten()\n",
    "A = discrete_transition_matrix(1, 1, 0.1)\n",
    "A_s = discrete_transition_matrix(2, 1, 0.1)\n",
    "print(f\"x1: {x1}, x2: {x2}\")\n",
    "print(f\"x_s: {x_s}\")\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"A_s:\\n{A_s}\")\n",
    "print(f\"Ax1: {A@x1}, Ax2: {A@x2}\")\n",
    "print(f\"Ax1, Ax2 stacked: {jnp.stack([A@x1, A@x2]).T.flatten()}\")\n",
    "print(f\"A_s x_s: {A_s@x_s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f26cfb-7454-4f9e-863b-a27d9cf8550d",
   "metadata": {},
   "source": [
    "Essentially, this means that all of the stacked operators\n",
    "just operate _per dimension_ as they would normally\n",
    "in the one-dimensional case.\n",
    "\n",
    "- This means that we don't need to care about $d$ anywhere,\n",
    "  as long as we stack the state and the operators correctly,\n",
    "  the math stays exactly the same.\n",
    "- To my understanding, this also means that every dimension\n",
    "  is modeled independently of each other\n",
    "  - For example, we don't get covariances between different dimensions,\n",
    "    or rather, it is zero\n",
    "  - In the prior, every dimension is a $q$-times IWP independently\n",
    "    of every other dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafca2c9-f82b-4b49-b66d-adbd070f33b9",
   "metadata": {},
   "source": [
    "#### ODE Filter\n",
    "\n",
    "Now, we can build our first probabilistic ODE solver\n",
    "- with constant stepsize $h$\n",
    "- no error estimation etc.\n",
    "- lots of other points we can adress later\n",
    "\n",
    "A simple version of an ODE filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1726f3-16b3-425f-bdf7-65314aacefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_filter(y0, f, f_args,\n",
    "               t0, t1,\n",
    "               q,  # order\n",
    "               h,\n",
    "               ekf_order=1):\n",
    "    h, t0 = jnp.array(h), jnp.array(t0)  # Ensure these are arrays\n",
    "    d = y0.shape[-1]  # dimension of y\n",
    "    # Initialization\n",
    "    x0 = initial_state(f, t0, y0, q, f_args)\n",
    "    stddev0 = jnp.zeros_like(y0)\n",
    "    P0 = jnp.diag(jnp.repeat(stddev0, q + 1))  # Assume zero initial cov.\n",
    "    H0, H = ssm_projection_matrices(d, q)\n",
    "    # R=0 -> dirac measure on initial value\n",
    "    R = jnp.zeros((d, d))\n",
    "    # Init. objects for storing results\n",
    "    ts, ys, stddevs = [t0], [y0], [stddev0]\n",
    "    filtering_means, filtering_covs, predictive_means, predictive_covs = [x0], [P0], [], []\n",
    "    # Init. loop variables\n",
    "    t_prev = t0\n",
    "    m_f_prev, P_f_prev = x0, P0  # Filtering parameters (of previous step)\n",
    "\n",
    "    # The ODE filter loop\n",
    "    while t_prev < t1:\n",
    "        m_f, P_f, m_p, P_p, y, stddev = \\\n",
    "            ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                            t_prev, h, d, q,\n",
    "                            H0, H, R, ekf_order)\n",
    "        # Store results\n",
    "        t = t_prev + h\n",
    "        ts.append(t)\n",
    "        ys.append(y)\n",
    "        stddevs.append(stddev)\n",
    "        filtering_means.append(m_f)\n",
    "        filtering_covs.append(P_f)\n",
    "        predictive_means.append(m_p)\n",
    "        predictive_covs.append(P_p)\n",
    "        # Update loop variables\n",
    "        t_prev, m_f_prev, P_f_prev = t, m_f, P_f\n",
    "\n",
    "    # Return results in dict\n",
    "    return {\n",
    "        \"ts\": jnp.asarray(ts).squeeze(),\n",
    "        \"ys\": jnp.asarray(ys).squeeze(),\n",
    "        \"stddevs\": jnp.asarray(stddevs).squeeze(),\n",
    "        \"predictive_means\": predictive_means,\n",
    "        \"predictive_covs\": predictive_covs,\n",
    "        \"filtering_means\": filtering_means,\n",
    "        \"filtering_covs\": filtering_covs,\n",
    "        \"H0\": H0, \"d\": d, \"q\": q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cded317-c72b-41a4-a5cb-0efcb67d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 1\n",
    "h = 1e-1  # If we decrease the stepsize, this is very accurate already\n",
    "#h = 1e-2\n",
    "ekf_order = 1\n",
    "f_sol = ode_filter(\n",
    "    y0, lotka_volterra_vf, lv_args,\n",
    "    t0, t1, q, h, ekf_order=ekf_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e5226-4cfa-498e-b1eb-c4c6ea621716",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lv_with_uncertainty(f_sol, c_sol=sol, title=f\"ODE Filter, q={q}, h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41fd74-38ac-47e6-b983-7059258d4a6e",
   "metadata": {},
   "source": [
    "### Smoother\n",
    "\n",
    "Great, our filter seems to be working!\n",
    "\n",
    "But, it only calculates the filtering distribution $p(x_n | z_{1:n})$.\n",
    "\n",
    "As we can see above, this is pretty useful already (for smaller stepsizes than used above, it could actually provide a decently accurate solution).\n",
    "However, at every $x_n$, it only considers information up to that time point.\n",
    "\n",
    "We can also perform a smoothing pass, where we go backwards in time\n",
    "to obtain the full smoothing posterior\n",
    "$$ p(x_n | z_{1:N}) = \\mathcal{N}(x_n; m_n^S, P_n^S) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6a86b-e329-417f-bb89-427869616581",
   "metadata": {},
   "source": [
    "#### Smoother step\n",
    "\n",
    "We start with $m_N^S = m_N^f, P_N^S = P_N^f$ (ie. set the last smoothing parameters to the last filtering parameters), and then iteratively perform the step\n",
    "\n",
    "\\begin{align*}\n",
    "    G_n &= P_n^f A(h_{n+1})^\\top \\left(P_{n+1}^p\\right)^{-1} &\\text{Gain} \\\\\n",
    "    m_n^S &= m_n^f + G_n(m_{n+1}^S - m_{n+1}^p) &\\text{smoothing mean} \\\\\n",
    "    P_n^S &= P_n^f + G_n(P_{n+1}^S - P_{n+1}^p) G_n^\\top &\\text{smoothing cov}\\,,\n",
    "\\end{align*}\n",
    "\n",
    "which we can easily implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127f39a-336d-4fdd-924e-453c68093177",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"d\", \"q\"])\n",
    "def smoother_step(\n",
    "        m_f, P_f,\n",
    "        m_p_next, P_p_next,\n",
    "        m_s_next, P_s_next,\n",
    "        H0, h, d, q):\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    G = P_f @ A.T @ jnp.linalg.inv(P_p_next)  # again, we'll improve this later.\n",
    "    m_s = m_f + G @ (m_s_next - m_p_next)  # posterior smoothing mean\n",
    "    P_s = P_f + G @ (P_s_next - P_p_next) @ G.T  # posterior smoothing cov\n",
    "    y_s = H0 @ m_s  # Again extract current y and stddev\n",
    "    stddev_s = jnp.sqrt(jnp.diag(H0 @ P_s @ H0.T))\n",
    "    return m_s, P_s, y_s, stddev_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88666c1b-a07a-49ef-bc3c-8d62b8ddd6aa",
   "metadata": {},
   "source": [
    "And can already implement the our ODE smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8ce40-b36b-42a1-aa85-d3c1081d0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_smoother(f_out):\n",
    "    # Setup, retrieve parameters\n",
    "    predictive_means = f_out[\"predictive_means\"]\n",
    "    predictive_covs = f_out[\"predictive_covs\"]\n",
    "    filtering_means = f_out[\"filtering_means\"]\n",
    "    filtering_covs = f_out[\"filtering_covs\"]\n",
    "    ts = f_out[\"ts\"]\n",
    "    N, d, q = len(ts), f_out[\"d\"], f_out[\"q\"]\n",
    "    H0 = f_out[\"H0\"]\n",
    "    # Init. loop variables\n",
    "    m_s_next = filtering_means[-1]\n",
    "    P_s_next = filtering_covs[-1]\n",
    "    smoothing_ts = [ts[-1]]  # Init. last smoothing parameters as last filtering parameters\n",
    "    smoothing_ys = [f_out[\"ys\"][-1]]\n",
    "    smoothing_stddevs = [f_out[\"stddevs\"][-1]]\n",
    "\n",
    "    for n in range(N - 2, -1, -1):  # shift by 1 due to zero-based indexing\n",
    "        # [N - 2, 0]\n",
    "        # (N-1 is last, and that step was already initialized as the\n",
    "        # last filtering parameters).\n",
    "        # Predictive params hold one less element at the start,\n",
    "        # so eg predictive_means[n] actually corresponds to m_p[n+1]\n",
    "        m_p_next, P_p_next = predictive_means[n], predictive_covs[n]\n",
    "        m_f, P_f = filtering_means[n], filtering_covs[n]\n",
    "        t = ts[n]\n",
    "        h = ts[n + 1] - t\n",
    "        m_s, P_s, y, stddev = smoother_step(\n",
    "            m_f, P_f, m_p_next, P_p_next, m_s_next, P_s_next, H0, h, d, q)\n",
    "        # Store results\n",
    "        smoothing_ts.insert(0, t)\n",
    "        smoothing_ys.insert(0, y)\n",
    "        smoothing_stddevs.insert(0, stddev)\n",
    "        m_s_next, P_s_next = m_s, P_s\n",
    "\n",
    "    return {\n",
    "            \"ts\": jnp.asarray(smoothing_ts).squeeze(),\n",
    "            \"ys\": jnp.asarray(smoothing_ys).squeeze(),\n",
    "            \"stddevs\": jnp.asarray(smoothing_stddevs).squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffaf4d-0064-47f1-902d-260bd99e30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_sol = ode_smoother(f_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfca2e2-8957-4061-9f27-eea9f667490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter_and_smoother(f_sol, s_sol, c_sol=sol, title=f\"ODE Filter and Smoother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e906683-cb1e-458f-a487-e3a97add1c83",
   "metadata": {},
   "source": [
    "### When to choose which?\n",
    "\n",
    "We have now essentially implemented 4 algorithms at once:\n",
    "- EKF0, EKS0, EKF1, EKS1\n",
    "  - 0 or 1 determines constant or linear approximation of $f$ in the filter\n",
    "  - EKF means filter only, EKS uses filter + smoother\n",
    "\n",
    "The most accurate will likely be EKS1, due to the smoothing pass and the more accurate approximation of $f$.\n",
    "\n",
    "But using one of the other variants could also be a good choice:\n",
    "- If evaluating the jacobian of $f$ is expensive, EKS0 or EKF0 could be a reasonable choice\n",
    "- If solving the ODE is expensive already/requires many steps, EKF0/1 could be a reasonable choice, as\n",
    "  - The smoothing pass requires as many steps as the filtering path and\n",
    "  - The filter itself can be implemented with constant memory (if you don't need to save every step for the smoother)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9dfb5-89fb-4ab4-b929-b48804331ded",
   "metadata": {},
   "source": [
    "## Extending the simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33fa19-a131-4c1b-842e-69febcb067ce",
   "metadata": {},
   "source": [
    "### Uncertainty estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684c151-be78-41e5-85dd-c9ed3bc061a4",
   "metadata": {},
   "source": [
    "#### Calibrated Local Uncertainty\n",
    "\n",
    "Remember the IWP scale $\\sigma$ in [Dynamic model / State](#Dynamic-model-/-State)?\n",
    "\n",
    "So far, we've just used $\\sigma = 1$.\n",
    "\n",
    "We can get a _local_ (quasi-ML) estimate $\\hat{\\sigma}_n^2$ via\n",
    "$$ \\hat{\\sigma}_n^2 = \\hat{z}_n^\\top \\left(H \\tilde{Q}(h_n) H^\\top \\right)^{-1} \\hat{z}_n^\\top\\,, $$\n",
    "where $\\tilde{Q}(h_n)$ is $Q(h_n)$ with $\\sigma^2 = 1$\n",
    "(as, remember, $Q(h_n)$ does depend on $\\sigma^2$).\n",
    "\n",
    "This is actually what the function ```discrete_diffusion_matrix(d, q, h)``` does\n",
    "(and the reason why we ignored the factor $\\sigma^2$).\n",
    "\n",
    "So, for calibrated (local) uncertainty, we can just run the filter like before,\n",
    "and, after we've calculated $\\hat{z}_n$, we can get our uncertainty estimate $\\hat{\\sigma}_n^2$,\n",
    "and just rescale $Q(h_n) = \\hat{\\sigma}_n^2 \\tilde{Q}(h_n)$, and have calibrated local uncertainty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e48aa-7f01-49ee-9328-19c1d49d9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_uncertainty(z_hat, H, Q):\n",
    "    sigma_sq = z_hat.T @ jnp.linalg.inv(H @ Q @ H.T) @ z_hat\n",
    "    return sigma_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9563ebd-28b9-4176-a76e-24f87d0d80d4",
   "metadata": {},
   "source": [
    "#### Local Error Estimation\n",
    "\n",
    "Additionally, we can now use our calibrated $Q(h_n) = \\hat{\\sigma}_n^2 \\tilde{Q}(h_n)$\n",
    "to obtain an estimate for the local error with\n",
    "$$ D(h_n) = \\sqrt{\\tilde{H} Q(h_n) \\tilde{H}^\\top}\\,, $$\n",
    "which is essentially just the covariance of $\\hat{z}_n$.\n",
    "\n",
    "If we have a multi-dimensional ODE, then the above estimates an error per dimension.\n",
    "\n",
    "I just took the max error as error estimate\n",
    "- Although I'm not sure this is the best way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481748a-be73-49cf-814c-c2d0dd096a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_error(H_hat, Q):\n",
    "    err = jnp.sqrt((H_hat @ Q @ H_hat.T).max())\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95104f32-8398-47b9-925a-0a700528145d",
   "metadata": {},
   "source": [
    "#### Step size adaptation\n",
    "\n",
    "Being able to adapt the step size $h_n$ is very beneficial:\n",
    "- For some steps, the solver could have to use a very small step size\n",
    "  - Especially so for _stiff_ ODEs\n",
    "  - Potentially so small that solving over an interval of interest\n",
    "    could become almost infeasible\n",
    "- Whereas in other intervals, the solver can get away with taking\n",
    "  relatively large steps\n",
    "  - Huge performance gains possible\n",
    "\n",
    "For example, the basic solver we constructed earlier is able to solve\n",
    "the Lotka-Volterra system pretty accurately, but we'd need a smaller step size,\n",
    "eg. $h = 0.01$, instead of $h = 0.1$, but:\n",
    "- This then takes 10 times longer,\n",
    "  - and requires 10 times as much memory (if using the smoother or not saving only at fixed\n",
    "    points)\n",
    "\n",
    "This is often done by specifying a (relative) tolerance, and trying to take\n",
    "steps as large as possible while keeping the error within tolerance.\n",
    "\n",
    "For our implementation, this means:\n",
    "- Perform the filter step as usual, but\n",
    "  - report the local error and\n",
    "  - computing the next step size based on the current local error\n",
    "- If the error is too large, we _reject_ the step,\n",
    "  and try again using the step size from above (that would otherwise have been used for the\n",
    "  next step)\n",
    "\n",
    "There are many possibilities for computing the next step size,\n",
    "we implemented _proportional control_:\n",
    "$$ h_n^{\\text{next}} = h_n \\rho \\left(\\frac{\\varepsilon}{D(h_n)}\\right)^{\\frac{1}{q+1}}\\,, $$\n",
    "where $\\epsilon$ is the tolerance, and $\\rho \\in (0, 1]$ is a safety factor.\n",
    "\n",
    "We use:\n",
    "- $\\rho = 0.9$\n",
    "- And we clamp $0.2 \\leq \\rho \\left(\\frac{\\varepsilon}{D(h_n)}\\right)^{\\frac{1}{q+1}} \\leq 10 $\n",
    "- (These are the same as in the 2021 paper listed in the references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449826b8-9e73-40e5-b622-c1c246cd4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_stepsize(h, q, err, reltol,\n",
    "                  stepsize_safety_factor,\n",
    "                  stepsize_min_change, stepsize_max_change):\n",
    "    stepsize_factor = (\n",
    "        stepsize_safety_factor\n",
    "        * (reltol / err) ** (1.0 / (q + 1.0)))\n",
    "    stepsize_factor = jax.lax.clamp(\n",
    "        stepsize_min_change, stepsize_factor, stepsize_max_change)\n",
    "    return h * stepsize_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef42f1c-9cbb-481a-9b62-3c2729ee1a7e",
   "metadata": {},
   "source": [
    "Let's add all of these changes to ```ode_filter()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f6e94-4d9f-4781-bf5a-a16f1bf434ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"f\", \"f_args\", \"d\", \"q\", \"ekf_order\"])\n",
    "def ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                    t_prev, h, d, q,\n",
    "                    H0, H, R, ekf_order,\n",
    "                    reltol, stepsize_safety_factor,\n",
    "                    stepsize_min_change, stepsize_max_change):\n",
    "    t = t_prev + h\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    Q = discrete_diffusion_matrix(d, q, h)\n",
    "\n",
    "    # predict (mean only)\n",
    "    m_p = A @ m_f_prev\n",
    "    # residual\n",
    "    z_hat, H_hat = residual(f, t, m_p, H, H0, f_args, ekf_order)\n",
    "\n",
    "    # local uncertainty est.\n",
    "    sigma_sq = local_uncertainty(z_hat, H, Q)\n",
    "    Q = sigma_sq * Q  # update Q\n",
    "    err = local_error(H_hat, Q)  # est. local error\n",
    "    h_next = next_stepsize(h, q, err, reltol,\n",
    "                           stepsize_safety_factor,\n",
    "                           stepsize_min_change, stepsize_max_change)\n",
    "\n",
    "    P_p = A @ P_f_prev @ A.T + Q  # predictive cov with updated Q\n",
    "    m_f, P_f = next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R)\n",
    "\n",
    "    y = H0 @ m_f\n",
    "    stddev = jnp.sqrt(jnp.diag(H0 @ P_f @ H0.T))\n",
    "    return m_f, P_f, m_p, P_p, y, stddev, err, h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a15398-c7cb-42a1-a151-2db86634becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With optional adaptive step size and calibrated uncertainty\n",
    "def ode_filter(y0, f, f_args,\n",
    "               t0, t1,\n",
    "               q,  # order\n",
    "               h0,\n",
    "               adaptive_stepsize=True,\n",
    "               reltol=1e-3,\n",
    "               stepsize_safety_factor=0.9,\n",
    "               stepsize_min_change=0.2,\n",
    "               stepsize_max_change=10.0,\n",
    "               ekf_order=1):\n",
    "    h0, t0 = jnp.array(h0), jnp.array(t0)  # Ensure these are arrays\n",
    "    reltol = jnp.array(reltol)\n",
    "    stepsize_safety_factor = jnp.array(stepsize_safety_factor)\n",
    "    stepsize_min_change = jnp.array(stepsize_min_change)\n",
    "    stepsize_max_change = jnp.array(stepsize_max_change)\n",
    "    d = y0.shape[-1]  # dimension of y\n",
    "    # Initialization\n",
    "    x0 = initial_state(f, t0, y0, q, f_args)\n",
    "    stddev0 = jnp.zeros_like(y0)\n",
    "    P0 = jnp.diag(jnp.repeat(stddev0, q + 1))  # Assume zero initial cov.\n",
    "    H0, H = ssm_projection_matrices(d, q)\n",
    "    # R=0 -> dirac measure on initial value\n",
    "    R = jnp.zeros((d, d))\n",
    "    # Init. objects for storing results\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    ts, ys, stddevs = [t0], [y0], [stddev0]\n",
    "    filtering_means, filtering_covs, predictive_means, predictive_covs = [x0], [P0], [], []\n",
    "    # Init. loop variables\n",
    "    t_prev, h = t0, h0\n",
    "    m_f_prev, P_f_prev = x0, P0  # Filtering parameters (of previous step)\n",
    "\n",
    "    # ODE filter loop\n",
    "    while t_prev < t1:\n",
    "        m_f, P_f, m_p, P_p, y, stddev, err, h_next = \\\n",
    "            ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                            t_prev, h, d, q,\n",
    "                            H0, H, R, ekf_order,\n",
    "                            reltol, stepsize_safety_factor,\n",
    "                            stepsize_min_change, stepsize_max_change)\n",
    "        # Here's the important change\n",
    "        if adaptive_stepsize:\n",
    "            if err > reltol:\n",
    "                # Reject current step, try again\n",
    "                h = h_next\n",
    "                rejected_steps += 1\n",
    "                continue\n",
    "        else:\n",
    "            # Do not adapt step size\n",
    "            h_next = h\n",
    "        accepted_steps += 1\n",
    "        # Store results\n",
    "        t = t_prev + h\n",
    "        ts.append(t)\n",
    "        ys.append(y)\n",
    "        stddevs.append(stddev)\n",
    "        filtering_means.append(m_f)\n",
    "        filtering_covs.append(P_f)  # These should be calibrated now\n",
    "        predictive_means.append(m_p)\n",
    "        predictive_covs.append(P_p)\n",
    "        # Update loop variables\n",
    "        t_prev, m_f_prev, P_f_prev = t, m_f, P_f\n",
    "        # Update step size\n",
    "        h = h_next\n",
    "\n",
    "    # Return results in dict\n",
    "    return {\n",
    "        \"ts\": jnp.asarray(ts).squeeze(),\n",
    "        \"ys\": jnp.asarray(ys).squeeze(),\n",
    "        \"stddevs\": jnp.asarray(stddevs).squeeze(),\n",
    "        \"predictive_means\": predictive_means,\n",
    "        \"predictive_covs\": predictive_covs,\n",
    "        \"filtering_means\": filtering_means,\n",
    "        \"filtering_covs\": filtering_covs,\n",
    "        \"num_accepted\": accepted_steps, \"num_rejected\": rejected_steps,\n",
    "        \"H0\": H0, \"d\": d, \"q\": q}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9303940-7d19-4238-9676-876b0e301dc2",
   "metadata": {},
   "source": [
    "Note that none of the changes made so far change anything about the smoother,\n",
    "we can still use the implementation from earlier.\n",
    "\n",
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fa033-4b5f-4cde-928b-e67f61ddb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 1\n",
    "h0 = 1e-2  # Initial step size, shouldn't be too large but otherwise doesn't matter much\n",
    "ekf_order = 1\n",
    "f_adaptive_sol = ode_filter(\n",
    "    y0, lotka_volterra_vf, lv_args,\n",
    "    t0, t1, q, h0, ekf_order=ekf_order,\n",
    "    reltol=1e-2,\n",
    "    adaptive_stepsize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57060c74-3025-4357-be8b-58b73caac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rejected: {f_adaptive_sol[\"num_rejected\"]}, Accepted: {f_adaptive_sol[\"num_accepted\"]}\")\n",
    "n_stddevs = 60\n",
    "plot_lv_with_uncertainty(f_adaptive_sol, c_sol=sol, num_stddevs=n_stddevs, title=f\"ODE Filter (adaptive step size), q={q}, h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66efcde-387d-44a9-bd74-40860b66655a",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "Above, we didn't actually get an improvement, in fact,\n",
    "in that setting, the solver performs _worse_ than without\n",
    "adaptive step size.\n",
    "- By worse I mean that it does more steps and is less accurate than\n",
    "  without adaptive step size\n",
    "- This _could_ mean that there's an issue with the implementation\n",
    "\n",
    "If we use order $q=2$, then the solver can benefit from our added\n",
    "adaptive_stepsize option (improve over constant stepsize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39da121-4a61-43cb-8d2b-48564a0d9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2 # For order 2 (or higher), it works\n",
    "h0 = 1e-1  # Initial step size, shouldn't be too large but otherwise doesn' matter\n",
    "ekf_order = 1\n",
    "f_adaptive_sol = ode_filter(\n",
    "    y0, lotka_volterra_vf, lv_args,\n",
    "    t0, t1, q, h0, ekf_order=ekf_order,\n",
    "    reltol=1e-2,\n",
    "    adaptive_stepsize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3975346-e7e8-4a6a-9fc7-afd0b8650528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rejected: {f_adaptive_sol[\"num_rejected\"]}, Accepted: {f_adaptive_sol[\"num_accepted\"]}\")\n",
    "n_stddevs = 60\n",
    "plot_lv_with_uncertainty(f_adaptive_sol, c_sol=sol, num_stddevs=n_stddevs, title=f\"ODE Filter (adaptive step size), q={q}, h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf5c2f-8114-4438-8622-9626316ca9d5",
   "metadata": {},
   "source": [
    "### Improving numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a1be7c-c427-4c51-b0b1-1c16018a955e",
   "metadata": {},
   "source": [
    "The covariance matrices (and diffusion matrix $Q$)\n",
    "are a big source of instability, because:\n",
    "\n",
    "- eg. $Q(h) = Q(h)_{i,j} = \\sigma^2 \\frac{h^{2q + 3 - i - j}}{(2q + 3 - i - j)(q + 1 - i)!(q +1 - j)!}$ is very ill-conditioned\n",
    "- Due to numerical inaccuracy, it might happen that the covariance matrices could have\n",
    "  negative eigenvalues, ie. are not positive definite anymore\n",
    "- We also need to invert them in some places\n",
    "\n",
    "Here are two ways to improve stability of the solver:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d259c-158d-4f76-a7b9-27a035b8a2d6",
   "metadata": {},
   "source": [
    "#### Nordsieck Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bc094-99aa-45cf-a872-3d7703bff071",
   "metadata": {},
   "source": [
    "We re-scale the state space at every step by\n",
    "$$ T_n = \\sqrt{h_n} \\text{diag}\\begin{pmatrix} \\frac{h^q}{q!} & \\frac{h^{q-1}}{(q-1)!} & \\cdots & h & 1 \\end{pmatrix}\\,, $$\n",
    "so eg. we model\n",
    "$$ p(x_{n+1} | x_n) = \\mathcal{N}(x_{n+1}; T_n A(h_n) T_n^{-1} x_n, T_n Q(h_n) T_n^\\top)\\,. $$\n",
    "This means that we store $x_n$ normally, but whenever we do inference,\n",
    "we scale it by $\\bar{x}_n = T_n^{-1} x_n$ to work in the transformed space instead,\n",
    "and then re-scale it back at the end by multiplying with $T_n$.\n",
    "\n",
    "One of the advantages of performing inference in the scaled space is that\n",
    "\\begin{align*}\n",
    "    A(h_n) &= T_n \\bar{A} T_n^{-1} \\\\\n",
    "    \\bar{A}_{i,j} &= \\begin{pmatrix} q + 1 - i \\\\ q + 1 - j \\end{pmatrix}\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "    Q(h_n) &= T_n \\bar{Q} T_n^\\top \\\\\n",
    "    \\bar{Q}_{i,j} &= \\frac{1}{2q + 3 - i - j}\\,,\n",
    "\\end{align*}\n",
    "so we can use $\\bar{A}, \\bar{Q}$ in the transformed space instead.\n",
    "\n",
    "Particularly, $\\bar{Q}$ is _way_ less ill-conditioned than before,\n",
    "and both $\\bar{A}$ and $\\bar{Q}$ do not depend on the step size anymore.\n",
    "\n",
    "New constructors for $\\bar{A}$ and $\\bar{Q}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87b862-b7f1-46f2-a64f-278f926af5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_transition_matrix_ns(d, q):\n",
    "    K = q - np.repeat(np.expand_dims(np.arange(q + 1), 0), q + 1, 0)\n",
    "    A = jnp.triu(scipy.special.comb(K.T, K)).astype(float)\n",
    "    return jnp.kron(A, jnp.eye(d))\n",
    "\n",
    "def discrete_diffusion_matrix_ns(d, q):\n",
    "    Q_j = jnp.stack((jnp.arange(q + 1) + 1,) * (q + 1))\n",
    "    Q = 1.0 / (2 * q + 3 - Q_j.T - Q_j)\n",
    "    return jnp.kron(Q, jnp.eye(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24217295-978a-46ee-9730-4c5c41b85e6d",
   "metadata": {},
   "source": [
    "And a function for computing $T_n$ and $T_n^{-1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c3e5f-8094-4578-bba7-b90be42dfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nordsieck_coord_transformation(d, q, h):\n",
    "    qs = jnp.flip(jnp.arange(q + 1), 0)\n",
    "    T_diag = jnp.sqrt(h) * (h ** qs / jax.scipy.special.factorial(qs))\n",
    "    T_inv_diag = 1.0 / T_diag\n",
    "    T = jnp.kron(jnp.diag(T_diag), jnp.eye(d))\n",
    "    T_inv = jnp.kron(jnp.diag(T_inv_diag), jnp.eye(d))\n",
    "    return T, T_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd25c0-5f3d-4d7d-b337-b5f26212abbf",
   "metadata": {},
   "source": [
    "As $f$ works in the non-transformed space, we of course also have to\n",
    "de-scale before evaluating $f$, and re-scale afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b7d9e-4753-4feb-bd4a-476488f0c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(f, t, m_p, H, H0, T, f_args, ekf_order):\n",
    "    m_p0 = H0 @ T @ m_p  # de-scale for f\n",
    "    f_at_m_p = f(t, m_p0, f_args)\n",
    "    if ekf_order == 0:\n",
    "        H_hat = H\n",
    "    else:\n",
    "        J_f = jax.jacfwd(f, argnums=1)(t, m_p0, f_args)\n",
    "        H_hat = H - J_f @ H0  # regular * in 1-d case\n",
    "    z_hat = f_at_m_p - H @ T @ m_p  # residual\n",
    "    H_hat = H_hat @ T\n",
    "    return z_hat, H_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d071e-9e7d-41ba-9e01-9b02fd669eef",
   "metadata": {},
   "source": [
    "#### Square-Root Kalman Filter\n",
    "\n",
    "The problem regarding the positive definite-ness of the covariance matrices\n",
    "can be mitigated by not working on them directly,\n",
    "but instead on their matrix square-roots. For example:\n",
    "$$ Q = L_Q L_Q^\\top $$\n",
    "\n",
    "The Cholesky decomposition presents itself as a useful tool for this\n",
    "($L_Q L_Q^\\top$ is a Cholesky decomposition of $Q$),\n",
    "it factorizes a (real) matrix into this form, such that $L_Q$ is lower-triangular.\n",
    "\n",
    "We can now formulate the Kalman filter using only the triangular factors $L_Q$\n",
    "(same as for all other covariance matrices we work on),\n",
    "which guarantees that eg. $L_Q L_Q^\\top = Q$ will always be symmetric positive definite\n",
    "whenever we want to reconstruct it.\n",
    "\n",
    "On top of that, for all matrices we had to invert before,\n",
    "we also get a decomposition into triangular matrices\n",
    "- We can compute the inverse of that factor by forward/backward-substitution\n",
    "- This is faster _and_ more stable than directly inverting the original matrix\n",
    "\n",
    "For example, the predictive mean and covariance during the filter step can be calculated like this:\n",
    "\\begin{align*}\n",
    "    \\bar{m}_n^f &= T_n^{-1} m_n^f  &\\text{scale filtering mean} \\\\\n",
    "    \\bar{L}_n^f &= T_n^{-1} L_n^f  &\\text{scale filtering cov factor} \\\\\n",
    "    \\bar{m}_{n+1}^p &= \\bar{A} \\bar{m_n}^f  &\\text{next predictive mean} \\\\\n",
    "    \\begin{pmatrix} \\bar{A}\\bar{L}_n^f & L_Q \\end{pmatrix}^\\top &= XR_{n+1} &\\text{see below}\\\\\n",
    "    \\bar{L}_{n+1}^p &= (R_{n+1})_{0:d(q+1)}^\\top &\\text{next predictive cov factor}\n",
    "\\end{align*}\n",
    "Where $XR_{n+1}$ is a QR-decomposition of $\\begin{pmatrix} \\bar{A}\\bar{L}_n^f & L_Q \\end{pmatrix}^\\top$, of which we don't need $X$, and $\\bar{L}_{n+1}^p$ is the\n",
    "top $(d(q+1)\\times d(q+1))$ block of $R_{n+1}$  (transposed, as due to the QR-decomposition,\n",
    "$R_{n+1}$ is upper-triangular).\n",
    "\n",
    "It can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3934f-18eb-4ef4-bdaf-9dc146f2db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(m_f, A, L_f, L_Q, T_inv):\n",
    "    m_f = T_inv @ m_f\n",
    "    L_f = T_inv @ L_f\n",
    "    m_p = A @ m_f\n",
    "    C_p_pre = jnp.vstack([(A @ L_f).T, L_Q.T])  # the stack (AL, L_Q)^\\top\n",
    "    L_p = jax.scipy.linalg.qr(C_p_pre, mode=\"economic\")[1].T\n",
    "    return m_p, L_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c9bc7-d74f-48b1-9b8d-dcdc19b8d15f",
   "metadata": {},
   "source": [
    "### Final implementation including all of the listed improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00dc4b-3b91-44c6-979b-859a88fb14f5",
   "metadata": {},
   "source": [
    "#### ODE Filter\n",
    "\n",
    "(Assuming $R=0$ for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6fa4f-93f5-4306-9ae3-becf96c33cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cho_cov_to_stddev(L, H0):\n",
    "    # Helper function for extracting sttdev from cholesky cov factor\n",
    "    return jnp.sqrt(jnp.diag(H0 @ L @ L.T @ H0.T))\n",
    "\n",
    "def predictive_cov(A, L_f, L_Q):\n",
    "    C_p_pre = jnp.vstack([(A @ L_f).T, L_Q.T])\n",
    "    L_p = jax.scipy.linalg.qr(C_p_pre, mode=\"economic\")[1].T\n",
    "    return L_p\n",
    "\n",
    "def local_error(L_Q, H_hat):\n",
    "    return jnp.sqrt((H_hat @ L_Q @ L_Q.T @ H_hat.T).max())\n",
    "\n",
    "def next_filtering_mean_and_cov(m_p, L_p, z_hat, H_hat):\n",
    "    # Kalman filter statistics\n",
    "    S_pre = (H_hat @ L_p).T\n",
    "    L_S = jax.scipy.linalg.qr(S_pre, mode=\"economic\")[1].T  # innovation cov factor\n",
    "    C_cross = L_p @ L_p.T @ H_hat.T\n",
    "    # invert L_S by solving a linear system,\n",
    "    # ie backward substitution as it's lower triangular\n",
    "    L_S_inv = jax.scipy.linalg.solve_triangular(L_S, jnp.eye(L_S.shape[0]), lower=True)\n",
    "    K = C_cross @ L_S_inv.T @ L_S_inv  # Kalman gain\n",
    "    # Next filtering mean/cov\n",
    "    m_f = m_p + K @ z_hat  # + or plus here?\n",
    "    L_f = (jnp.eye(L_p.shape[1]) - K @ H_hat) @ L_p\n",
    "    return m_f, L_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfaf809-c7df-464c-8079-b9bd4f8d8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"f\", \"f_args\", \"d\", \"q\", \"adaptive_stepsize\", \"ekf_order\"])\n",
    "def ode_filter_step(\n",
    "        m_f_prev, L_f_prev,\n",
    "        f, f_args,\n",
    "        t, h,\n",
    "        d, q,\n",
    "        A, L_Q_hat,  # pass A, L_Q as they are now constant\n",
    "        H0, H,\n",
    "        z_error_scale,\n",
    "        reltol,\n",
    "        adaptive_stepsize,\n",
    "        stepsize_safety_factor,\n",
    "        stepsize_min_change,\n",
    "        stepsize_max_change,\n",
    "        ekf_order):\n",
    "    T, T_inv = nordsieck_coord_transformation(d, q, h)\n",
    "    # scale previous filtering parameters\n",
    "    m_f_prev = T_inv @ m_f_prev\n",
    "    L_f_prev = T_inv @ L_f_prev\n",
    "\n",
    "    m_p = A @ m_f_prev  # predictive mean\n",
    "    z_hat, H_hat = residual(f, t, m_p, H, H0, T, f_args, ekf_order=ekf_order)\n",
    "    # See filter function for explanation of z_error_scale\n",
    "    sigma_hat = z_error_scale * z_hat.T @ z_hat\n",
    "    # Update L_Q with local uncertainty estimate\n",
    "    L_Q = jnp.sqrt(sigma_hat) * L_Q_hat\n",
    "    L_p = predictive_cov(A, L_f_prev, L_Q)\n",
    "\n",
    "    m_f, L_f = next_filtering_mean_and_cov(m_p, L_p, z_hat, H_hat)\n",
    "    m_f = T @ m_f  # de-scale output\n",
    "    L_f = T @ L_f\n",
    "    y = H0 @ m_f  # extract y and stddev\n",
    "    stddev = cho_cov_to_stddev(L_f, H0)\n",
    "\n",
    "    err = local_error(L_Q, H_hat)\n",
    "    h_next = next_stepsize(h, q, err, reltol,\n",
    "                           stepsize_safety_factor,\n",
    "                           stepsize_min_change, stepsize_max_change)\n",
    "\n",
    "    return m_f, L_f, m_p, L_p, y, stddev, err, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32968218-1cdd-4921-bf03-87bf9bc6da56",
   "metadata": {},
   "source": [
    "And we finally get the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e6061-217d-4a03-b32c-fcd7eba4c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_filter(\n",
    "        y0,\n",
    "        f, f_args,\n",
    "        t0, t1,\n",
    "        q, h0,\n",
    "        reltol=1e-3,\n",
    "        adaptive_stepsize=True,\n",
    "        stepsize_safety_factor=0.9,\n",
    "        stepsize_min_change=0.2,\n",
    "        stepsize_max_change=10.0,\n",
    "        ekf_order=1):\n",
    "    h0, t0 = jnp.array(h0), jnp.array(t0)  # Ensure these are arrays\n",
    "    reltol = jnp.array(reltol)\n",
    "    stepsize_safety_factor = jnp.array(stepsize_safety_factor)\n",
    "    stepsize_min_change = jnp.array(stepsize_min_change)\n",
    "    stepsize_max_change = jnp.array(stepsize_max_change)\n",
    "    d = y0.shape[-1]  # dimension of y\n",
    "    # Initialization\n",
    "    x0 = initial_state(f, t0, y0, q, f_args)\n",
    "    stddev0 = jnp.zeros_like(y0)\n",
    "    L_P0 = jnp.diag(jnp.repeat(stddev0, q + 1))  # Assume zero initial cov.\n",
    "    H0, H = ssm_projection_matrices(d, q)\n",
    "    # R=0 -> dirac measure on initial value\n",
    "    # Init. objects for storing results\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    ts, ys, stddevs = [t0], [y0], [stddev0]\n",
    "    filtering_means, filtering_covs, predictive_means, predictive_covs = [x0], [L_P0], [], []\n",
    "    # Init. loop variables\n",
    "    t_prev, h = t0, h0\n",
    "    m_f_prev, L_f_prev = x0, L_P0  # Filtering parameters (of previous step)\n",
    "\n",
    "    A = discrete_transition_matrix_ns(d, q)\n",
    "    Q = discrete_diffusion_matrix_ns(d, q)\n",
    "    L_Q = jnp.linalg.cholesky(Q)  # cholesky-factorize Q\n",
    "    # the (H @ Q(1) @ H)^{-1} part of the local error term\n",
    "    # reduces to the below error_scale * Id, so this simplifies a lot\n",
    "    z_error_scale = jnp.array(2 * q - 1.0)\n",
    "\n",
    "    # ODE filter loop\n",
    "    while t_prev < t1:\n",
    "        m_f, L_f, m_p, L_p, y, stddev, err, h_next = \\\n",
    "                ode_filter_step(m_f_prev, L_f_prev, f, f_args, t_prev, h, d, q,\n",
    "                                A, L_Q, H0, H,\n",
    "                                z_error_scale,\n",
    "                                reltol,\n",
    "                                adaptive_stepsize=adaptive_stepsize,\n",
    "                                stepsize_safety_factor=stepsize_safety_factor,\n",
    "                                stepsize_min_change=stepsize_min_change,\n",
    "                                stepsize_max_change=stepsize_max_change,\n",
    "                                ekf_order=ekf_order)\n",
    "        if adaptive_stepsize:\n",
    "            if err > reltol:\n",
    "                # Reject current step, try again\n",
    "                h = h_next\n",
    "                rejected_steps += 1\n",
    "                continue\n",
    "        else:\n",
    "            # Do not adapt step size\n",
    "            h_next = h\n",
    "        accepted_steps += 1\n",
    "        # Store results\n",
    "        t = t_prev + h\n",
    "        ts.append(t)\n",
    "        ys.append(y)\n",
    "        stddevs.append(stddev)\n",
    "        filtering_means.append(m_f)\n",
    "        filtering_covs.append(L_f)\n",
    "        predictive_means.append(m_p)\n",
    "        predictive_covs.append(L_p)\n",
    "        # Update loop variables\n",
    "        t_prev, m_f_prev, L_f_prev = t, m_f, L_f\n",
    "        # Update step size\n",
    "        h = h_next\n",
    "\n",
    "    # Format and return results\n",
    "    return {\n",
    "        \"ts\": jnp.asarray(ts).squeeze(),\n",
    "        \"ys\": jnp.asarray(ys).squeeze(),\n",
    "        \"stddevs\": jnp.asarray(stddevs).squeeze(),\n",
    "        \"predictive_means\": predictive_means,\n",
    "        \"predictive_covs\": predictive_covs,\n",
    "        \"filtering_means\": filtering_means,\n",
    "        \"filtering_covs\": filtering_covs,\n",
    "        \"num_accepted\": accepted_steps, \"num_rejected\": rejected_steps,\n",
    "        \"H0\": H0, \"d\": d, \"q\": q,\n",
    "        \"A\": A, \"L_Q\": L_Q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d02e79-5927-4640-825e-9434c1340224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: This has an error somewhere\n",
    "@partial(jax.jit, static_argnames=[\"d\", \"q\"])\n",
    "def smoother_step(\n",
    "        m_f, L_f,\n",
    "        m_p_next, L_p_next,\n",
    "        m_s_next, L_s_next,\n",
    "        A, L_Q, H0, h, d, q):\n",
    "    T, T_inv = nordsieck_coord_transformation(d, q, h)\n",
    "    # Apply the transformation\n",
    "    m_f = T_inv @ m_f\n",
    "    L_f = T_inv @ L_f\n",
    "    m_s_next = T_inv @ m_s_next\n",
    "    L_s_next = T_inv @ L_s_next\n",
    "    # Invert L_p more efficiently\n",
    "    L_p_inv = jax.scipy.linalg.solve_triangular(\n",
    "            L_p_next, jnp.eye(L_p_next.shape[0]), lower=True)\n",
    "    G = L_f @ (A @ L_f).T @ L_p_inv.T @ L_p_inv  # gain\n",
    "    m_s = m_f + G @ (m_s_next - m_p_next)  # posterior mean\n",
    "    # Smoothing cov factor\n",
    "    L_s_pre = jnp.hstack([\n",
    "        (jnp.eye(A.shape[0]) - G @ A) @ L_f,\n",
    "        G @ L_Q,\n",
    "        G @ L_s_next]).T\n",
    "    L_s = jax.scipy.linalg.qr(L_s_pre, mode=\"economic\")[1].T\n",
    "    # Transform back\n",
    "    m_s = T @ m_s\n",
    "    L_s = T @ L_s\n",
    "    # Extract y and stddev\n",
    "    y_s = H0 @ m_s\n",
    "    stddev_s = cho_cov_to_stddev(L_s, H0)\n",
    "    return m_s, L_s, y_s, stddev_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3d5c4-2dea-4dce-a115-2712e2e2a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_smoother(f_out):\n",
    "    # Setup, retrieve parameters\n",
    "    predictive_means = f_out[\"predictive_means\"]\n",
    "    predictive_covs = f_out[\"predictive_covs\"]\n",
    "    filtering_means = f_out[\"filtering_means\"]\n",
    "    filtering_covs = f_out[\"filtering_covs\"]\n",
    "    ts = f_out[\"ts\"]\n",
    "    N, d, q = len(ts), f_out[\"d\"], f_out[\"q\"]\n",
    "    A, L_Q, H0 = f_out[\"A\"], f_out[\"L_Q\"], f_out[\"H0\"]\n",
    "    # Init. loop variables\n",
    "    m_s_next = filtering_means[-1]\n",
    "    L_s_next = filtering_covs[-1]\n",
    "    smoothing_ts = [ts[-1]]  # Init. last smoothing parameters as last filtering parameters\n",
    "    smoothing_ys = [f_out[\"ys\"][-1]]\n",
    "    smoothing_stddevs = [f_out[\"stddevs\"][-1]]\n",
    "\n",
    "    for n in range(N - 2, -1, -1):\n",
    "        m_p_next, L_p_next = predictive_means[n], predictive_covs[n]\n",
    "        m_f, L_f = filtering_means[n], filtering_covs[n]\n",
    "        t = ts[n]\n",
    "        h = ts[n + 1] - t\n",
    "        m_s, L_s, y, stddev = smoother_step(\n",
    "            m_f, L_f, m_p_next, L_p_next, m_s_next, L_s_next, A, L_Q, H0, h, d, q)\n",
    "        # Store results\n",
    "        smoothing_ts.insert(0, t)\n",
    "        smoothing_ys.insert(0, y)\n",
    "        smoothing_stddevs.insert(0, stddev)\n",
    "        m_s_next, L_s_next = m_s, L_s\n",
    "\n",
    "    return {\n",
    "            \"ts\": jnp.asarray(smoothing_ts).squeeze(),\n",
    "            \"ys\": jnp.asarray(smoothing_ys).squeeze(),\n",
    "            \"stddevs\": jnp.asarray(smoothing_stddevs).squeeze()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b1cfa-b068-44a9-852e-5b7773b9c214",
   "metadata": {},
   "source": [
    "Test our final solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cf67a-da40-45d7-b148-2a22fc16d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 5   # Use 5-th order (which would have, in this case,\n",
    "        # also worked before improving numerical stability).\n",
    "h0 = 0.1\n",
    "ekf_order = 1\n",
    "f_stability_sol = ode_filter(\n",
    "    y0, lotka_volterra_vf, lv_args,\n",
    "    t0, t1, q, h0, ekf_order=ekf_order,\n",
    "    reltol=1e-3,\n",
    "    adaptive_stepsize=True)\n",
    "\n",
    "s_stability_sol = ode_smoother(f_stability_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95fc65-ef1d-4401-af01-2c7ca6d4ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rejected: {f_adaptive_sol[\"num_rejected\"]}, Accepted: {f_adaptive_sol[\"num_accepted\"]}\")\n",
    "n_stddevs_f = 2000\n",
    "n_stddevs_s = 100\n",
    "plot_lv_with_uncertainty(f_stability_sol, c_sol=sol, num_stddevs=n_stddevs_f,\n",
    "                         title=f\"ODE Filter (final implementation), q={q}, h={h}\")\n",
    "plot_filter_and_smoother(f_stability_sol, s_stability_sol, c_sol=sol, num_stddevs=n_stddevs_s,\n",
    "                         title=f\"ODE Filter and Smoother (final implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71de542-b42b-4768-a3eb-a7851fb2a82f",
   "metadata": {},
   "source": [
    "NOTE: There's an error somewhere in the smoother_step() function,\n",
    "but I wasn't able to find/fix it.\n",
    "I've included the code for the final smoother anyways;\n",
    "the error causes (as can be seen above in the __very__ different scaling of the uncertainty) the smoothing covariance\n",
    "to be too large (near beginning/end, ie. where it currently seems as if there were no uncertainty, it's actually correct, I believe it's too large everywhere else)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
