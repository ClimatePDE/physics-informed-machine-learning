{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26ba465-164b-4b0a-878f-9983ce189917",
   "metadata": {},
   "source": [
    "# Probabilistic ODE solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef66db9-73a6-4e81-86f2-c904fcff7f52",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "This notebook was created to accompany a presentation of\n",
    "probabilistic ODE solvers for the seminar \"Physics-informed Machine Learning\" at the University of Tübingen (winter term 2024/2025).\n",
    "\n",
    "It intends to explain the concept of probabilistic ODE solvers (ODE filters and smoothers) and will build a simple implementation of a probabilistic ODE solver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cff49c-8668-4f36-b78a-667ae1a09bf1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "This tutorial is based on:\n",
    "\n",
    "- Mostly, esp. the theoretical parts:\n",
    "  -  Philipp Hennig, Michael A. Osborne, and Hans P. Kersting\n",
    "    - Probabilistic Numerics: Computation as Machine Learning. Cambridge University Press, 2022\n",
    "    - DOI [10.1017/9781316681411](https://doi.org/10.1017/9781316681411)\n",
    "    - Also available for free personal use here: [https://www.probabilistic-numerics.org/textbooks/](https://www.probabilistic-numerics.org/textbooks/)\n",
    "- And also in parts:\n",
    "  - Filip Tronarp et al.\n",
    "    - Probabilistic Solutions to Ordinary Differential Equations as Nonlinear Bayesian Filtering: A New Perspective\n",
    "    - In: Statistics and Computing 29.6 (Nov. 2019), pp. 1297–1315. issn: 1573-1375.\n",
    "    - DOI [10.1007/s11222-019-09900-1](https://doi.org/10.1007/s11222-019-09900-1)\n",
    "- The latter parts on numerical stability and the implementation thereof is based on:\n",
    "  - Nathanael Bosch, Philipp Hennig, Filip Tronarp\n",
    "  - Calibrated Adaptive Probabilistic ODE Solvers\n",
    "  - On ArXiv, Feb. 2021\n",
    "  - DOI [10.48550/arXiv.2012.08202](https://doi.org/10.48550/arXiv.2012.08202)\n",
    "\n",
    "Where other sources are used, a reference/link is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25021b-c197-4251-8d7d-1207d3baea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies here (uncomment first)\n",
    "# cuda\n",
    "#!pip install numpy scipy matplotlib 'jax[cuda]' diffrax\n",
    "\n",
    "# no cuda\n",
    "#!pip install numpy scipy matplotlib jax diffrax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3255b2-d066-490e-8c94-f0a0483c52b9",
   "metadata": {},
   "source": [
    "JAX preallocates 75% of GPU memory.\n",
    "\n",
    "If you run into OOM errors at the start,\n",
    "you can try to circumvent this by running this process\n",
    "(ie. the jupyter server or whatever you use to run the notebook)\n",
    "with one of these environment variables:\n",
    "\n",
    "- XLA_PYTHON_CLIENT_MEM_FRACTION=.50 (% of preallocated mem; or set it even lower); or:\n",
    "- XLA_PYTHON_CLIENT_PREALLOCATE=false (disable preallocation entirely)\n",
    "\n",
    "See also: [JAX GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654733-dffe-4d6c-ab36-3aa8d4b5dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "import jax\n",
    "import scipy\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.experimental import jet\n",
    "from diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n",
    "# Use float64 precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "# higher plot resolution\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885f2e1-3995-4d4c-abfd-46c420108861",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3619ae3-db90-4a24-991c-fb77a628305d",
   "metadata": {},
   "source": [
    "### Ordinary Differential Equations (ODEs)\n",
    "\n",
    "_Here_, an ODE is a function\n",
    "$$ x'(t) = f(x(t)) \\in \\mathbb{R}^d\\,, t \\in [0, T]\\,, $$\n",
    "and we want to solve it by finding a curve\n",
    "$$ x: [0, T] \\rightarrow \\mathbb{R}^d $$\n",
    "with initial value\n",
    "$$ x(0) = x_0 $$\n",
    "that satisfies this equation.\n",
    "\n",
    "This means that, as opposed to general ODEs, we require,\n",
    "mainly to keep the theoretical parts simple:\n",
    "\n",
    "- The ODE only contains the first derivative $x'(t)$, no higher-order derivatives\n",
    "  - ie. we only consider _first-order_ ODEs\n",
    "  - This comes without loss of generality:\n",
    "    - all higher-order ODEs can be rewritten as first-order ODE\n",
    "    - But: the solver _could_ be extended to also consider higher-order ODE information\n",
    "- $f$ does not depend on $t$ ($\\rightarrow$ autonomous ODE)\n",
    "  - Also no loss of generality\n",
    "    - Again, every ODE can be transformed into autonomous form\n",
    "- The equation given above is in _explicit_ form\n",
    "  - we won't consider _implicit_ ODEs (ie. of form $ f(x(t), x'(t)) = 0 $)\n",
    "    - This is a loss of generality; the solver presented here works only for explicit ODEs\n",
    "- A known initial value, so we are actually interested in an ODE _initial value problem_ (IVP)\n",
    "  - I'll just refer to this as ODE anyways in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e5e38-b2ea-46c7-a2ff-12486c31cde0",
   "metadata": {},
   "source": [
    "#### Lotka-Volterra system\n",
    "\n",
    "The Lotka-Volterra system/equations model predator/prey-populations as a 2-dimensional ODE:\n",
    "\\begin{align*}\n",
    "    y_1'(t) &= \\alpha y_1(t) - \\beta y_1(t) y_2(t) \\\\\n",
    "    y_2'(t) &= -\\gamma y_2(t) + \\delta y_1(t) y_2(t)\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "- $y_1$ is the population density/size of the prey\n",
    "- $y_2$ is the population density of the predator\n",
    "  - $\\alpha$: prey growth rate\n",
    "  - $\\gamma$: predator death rate\n",
    "  - $\\beta$: effect of predator population density on prey death rate\n",
    "  - $\\delta$: effect of prey population density on predator growth rate\n",
    "\n",
    "For more details, see eg. [Lotka–Volterra equations (Wikipedia)](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations).\n",
    "\n",
    "Let's see an example of it, using a classical ODE solver\n",
    "(code taken from [Diffrax documentation](https://docs.kidger.site/diffrax/examples/coupled_odes/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda6fa6-e6b2-4e46-9554-78502e2b0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"args\"])\n",
    "def lotka_volterra_vf(t, y, args):\n",
    "    prey, predator = y[..., 0], y[..., 1]\n",
    "    alpha, beta, gamma, delta = args\n",
    "    d_prey: Array = alpha * prey - beta * prey * predator\n",
    "    d_predator: Array = -gamma * predator + delta * prey * predator\n",
    "    return jnp.array([d_prey, d_predator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5456d5-37df-45fd-bc37-d1ba7d0cde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0\n",
    "t1 = 140\n",
    "y0 = jnp.array([10.0, 10.0])\n",
    "lv_args = (0.1, 0.02, 0.4, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64416d46-903e-4aff-b398-02136f674f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = ODETerm(lotka_volterra_vf)\n",
    "solver = Tsit5()\n",
    "saveat = SaveAt(ts=jnp.linspace(t0, t1, 1000))\n",
    "dt0 = 0.1\n",
    "sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=lv_args, saveat=saveat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf08519-048c-4656-88e0-2c7ca763afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sol.ts, sol.ys, label=[\"Prey\", \"Predator\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bcec2-2421-44ef-a85a-70e9e7e73ec0",
   "metadata": {},
   "source": [
    "### Classical ODE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cf4b2-d296-460d-aae4-1ded22b0f4ee",
   "metadata": {},
   "source": [
    "#### How do they work?\n",
    "\n",
    "- Work on discrete time steps:\n",
    "  $$ 0 = t_0 < t_1 < \\ldots < t_N = T\\,, \\text{ with step sizes: } h_n = t_n - t_{n - 1} $$\n",
    "- Iteratively perform steps:\n",
    "  - At step $n$, we have $\\hat{x}(t_{n-1}) \\approx x(t_{n-1})$\n",
    "  - Estimate derivative $x'(t_{n-1}) = f(x(t_{n-1})) \\approx f(\\hat{x}(t_{n-1}))$\n",
    "    - Potentially use more information\n",
    "    - Eg. approximate higher-order derivatives\n",
    "  - Construct approximation to $\\hat{x}(t_n) = \\hat{x}(t_{n-1} + h_n)$\n",
    "    - Using derivative information/other information\n",
    "    - Example: Eulers method\n",
    "      - $\\hat{x}(t_{n-1} + h_n) \\approx \\hat{x}(t_{n-1}) + h_n f(\\hat{x}(t_{n-1}))$\n",
    "      - First-order Taylor expansion (linear approximation) of $\\hat{x}(t_{n-1} + h_n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c8e18-b18b-4a34-b85b-ec3cff4a6893",
   "metadata": {},
   "source": [
    "#### Order\n",
    "\n",
    "An ODE solver of order $q$\n",
    "\n",
    "- essentially fits a $q$-th order Taylor expansion\n",
    "- thus has _local_ error (or convergence rate) $\\mathcal{O}(h^{q+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647267f-4678-4397-9cc6-6cbe05f29b1c",
   "metadata": {},
   "source": [
    "### Probabilistic viewpoint: Viewing ODE solving as regression\n",
    "\n",
    "One can view solving an ODE as regression on\n",
    "$$ \\mathcal{D} = \\left\\{x(0) = x_0, x'(t_n) = f(\\hat{x}(t_n)) \\mid\n",
    "n \\in \\{0, \\ldots, N\\}\\right\\} $$\n",
    "(that is, all the data available to the solver),\n",
    "where the goal is to find a function $x$ that fulfills all of these equations,\n",
    "and the data is revealed to the solver progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306daf5-d841-4a66-8c45-62b958127b3e",
   "metadata": {},
   "source": [
    "## Probabilistic ODE solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961541a9-57fe-4237-8fec-5d26dbc6b7b3",
   "metadata": {},
   "source": [
    "### GP Regression\n",
    "\n",
    "Jointly model $y(t)$ and $y'(t)$ in a state vector:\n",
    "$$ \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix} = x(t) \\sim \\mathcal{GP}(\\bar{x}, k) $$\n",
    "with mean function $\\bar{x}(t)$ and kernel function $k(t_1, t_2)$.\n",
    "\n",
    "How to perform inference efficiently?\n",
    "- In complexity $\\mathcal{O}(N)$\n",
    "  - instead of $\\mathcal{O}(N^3)$ for traditional GP regression\n",
    "- How to take advantage of the derivative information?\n",
    "\n",
    "$\\rightarrow$ we need a sensible prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d3593-3a4e-436f-a3bb-e46fd490c61f",
   "metadata": {},
   "source": [
    "### Model ODE solution in Continuous-Time State Space Model (SSM)\n",
    "\n",
    "We can model the state vector $x(t) \\in \\mathbb{R}^{q}$ by a stochastic process:\n",
    "$$ x(t) \\sim X(t) $$\n",
    "and introduce projections $H_0, H \\in \\mathbb{R}^{d \\times (q+1)}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    y(t) &= H_0 x(t) \\\\\n",
    "    y'(t) &= H x(t) \\,.\n",
    "\\end{align*}\n",
    "\n",
    "Since we are interested in IVPs, we also model the initial value by\n",
    "$$ X(0) \\sim \\mathcal{N}(m_0, P_0) $$\n",
    "(more details later on).\n",
    "\n",
    "An example where $x$ models $y(t)$ and its first three derivatives (ie. $q = 3$):\n",
    "\n",
    "\\begin{align*}\n",
    "    H_0 &= \\begin{bmatrix} 1 & 0 & 0 & 0 \\end{bmatrix} \\\\\n",
    "    H &= \\begin{bmatrix} 0 & 1 & 0 & 0 \\end{bmatrix} \\\\\n",
    "    x(t) &= \\begin{pmatrix} y(t) \\\\ y'(t) \\\\ y''(t) \\\\ y'''(t) \\\\ \\end{pmatrix} \\sim X(t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937f9a1-3225-4309-9412-9fd222762720",
   "metadata": {},
   "source": [
    "We can build the projection matrices as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897014d-e38d-48a4-81ae-46f4fec93026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssm_projection_matrices(d, q):\n",
    "    # Projections: project SSM state -> y (H0) or SSM state -> y' (H)\n",
    "    H0 = jnp.zeros((1, q + 1)).at[0, 0].set(1.0)\n",
    "    H = jnp.zeros((1, q + 1)).at[0, 1].set(1.0)\n",
    "    Id = jnp.eye(d)  # Ignore this and below for now.\n",
    "    H0 = jnp.kron(H0, Id)\n",
    "    H = jnp.kron(H, Id)\n",
    "    return H0, H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627be9c3-f3dd-4b13-86fc-b9def4763e6f",
   "metadata": {},
   "source": [
    "#### Dynamic model / State\n",
    "\n",
    "We now let $X(t)$ follow this linear, time-invariant SDE:\n",
    "$$ dx(t) = Fx(t)dt + Ld\\omega_t $$\n",
    "where $ F = \\begin{bmatrix} 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\\\ 0&0&0&0 \\end{bmatrix} $\n",
    "and $ L = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\sigma \\end{pmatrix} $, and $d\\omega_t$\n",
    "can be informally thought of as \"increment of the Wiener process\".\n",
    "\n",
    "This gives rise to an interesting structure:\n",
    "\n",
    "- all elements in $x(t)$ are derivatives of each other for all $t$\n",
    "- $y'''(t)$ is modeled as Wiener process, ie. the random walk\n",
    "  (as its derivative is the increment of the Wiener process)\n",
    "\n",
    "Because:\n",
    "$$\n",
    "dx(t)\n",
    "= \\begin{bmatrix} 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\\\ 0&0&0&0 \\end{bmatrix}\n",
    "\\begin{pmatrix} y(t) \\\\ y'(t) \\\\ y''(t) \\\\ y'''(t) \\\\ \\end{pmatrix} dt\n",
    "+ \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\sigma \\end{pmatrix} d\\omega_t\n",
    "= \\begin{pmatrix} y'(t)dt \\\\ y''(t)dt \\\\ y'''(t)dt \\\\ d\\omega_t \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This means that $x(t)$ now models some function $y(t)$ and its $q=3$ first\n",
    "derivatives, and $y'''(t)$ is modeled as Wiener process/random walk.\n",
    "\n",
    "Thus, this Prior is known as the $q$-times integrated Wiener process (with scale $\\sigma$).\n",
    "\n",
    "Also, as is evident above, the $q$-times IWP prior is a GP that satisfies the Markov property\n",
    "(referred to as Gauss-Markov process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1914d9-4f3f-446d-bdb9-9511b975d573",
   "metadata": {},
   "source": [
    "#### Measurement model (Information operator)\n",
    "\n",
    "To our dynamic model, we now add a measurement model.\n",
    "\n",
    "We define:\n",
    "$$ z(t) \\sim Z(t) := g(X(t)) := HX(t) - f(H_0 X(t))\\,, $$\n",
    "so\n",
    "$$ z(t) = g(x(t)) = Hx(t) - f(H_0 x(t)) = y'(t) - f(y(t))\\,, $$\n",
    "which is referred to as the _state misalignment_.\n",
    "\n",
    "$Z$ now serves as _information operator_:\n",
    "It extracts information about the the state misalignment $z(t)$ from the ODE,\n",
    "quantifying the \"correctness\" of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c968fa-4ebf-42b0-82f9-f9240fbd62c5",
   "metadata": {},
   "source": [
    "#### Likelihood\n",
    "\n",
    "Now, we have our prior (the $q$-times IWP), and a measurement model (through the information operator).\n",
    "\n",
    "As seen above, $z(t) = y'(t) - f(y(t))$, which means that iff $y$ is a solution to the ODE (ie. $y'(t) = f(y(t))$ for all $t$; and of course also $y(0) = y_0$),\n",
    "then $z(t) = 0$.\n",
    "\n",
    "Thus, if we use the likelihood\n",
    "$$ p(z(t) \\mid x(t)) = \\delta(g(x(t))) = \\delta(Hx(t) - f(H_0 x(t))) = \\delta(y'(t) - f(y(t)))\\,, $$\n",
    "then, if we simply observe the data $0 = z(t)$ for all $t$,\n",
    "the posterior $p(x \\mid z = 0)$ contains the solution (also with zero uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee2e24-f9c1-4d43-9b66-4f13c5724146",
   "metadata": {},
   "source": [
    "### Discrete-time version of the SSM\n",
    "\n",
    "Of course, inference in this model is intractable,\n",
    "we can only work on discrete (and finitely many) points $t_n$;\n",
    "for tractable inference, we need to discretize the above continuous-time SSM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599bbf2-cce4-4ce4-adf7-9b1c7b445323",
   "metadata": {},
   "source": [
    "With functions of $t_n$ now denoted by subscript $n$ (eg. $x(t_n) := x_n$),\n",
    "the above can be discretized into\n",
    "\\begin{align}\n",
    "    p(x_0) &= \\mathcal{N}(x_0; m_0, P_0) &\\text{Initial value}\\\\\n",
    "    p(x_{n+1} \\mid x_n) &= \\mathcal{N}(x_{n+1}; A(h_{n+1})x_n, Q(h_{n+1})) &\\text{Dynamics}\\\\\n",
    "    p(z_n \\mid x_n) &= \\delta(z_n - g(x_n)) &\\text{Observations (information operator)}\n",
    "\\end{align}\n",
    "\n",
    "Or, alternatively,\n",
    "\\begin{align}\n",
    "    p(z_n \\mid x_n) &= \\mathcal{N}(z_n; g(x_n), R) &\\text{Observations (information operator)}\\,,\n",
    "\\end{align}\n",
    "which is less general (but the variance $R$ can help in some cases),\n",
    "and, if the vector field $f$ is linear, this is a linear Gaussian SSM (as $g$ then is linear too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb6c6f-dfc6-4684-b943-4a7a43f4af59",
   "metadata": {},
   "source": [
    "In the discretized SSM, we have $A(h_n), Q(h_n)$ given by\n",
    "\\begin{align*}\n",
    "    A(h_n)_{i,j} &= \\mathbb{I}(j \\geq i) \\frac{h_n^{j-i}}{(j-i)!} &\\text{Transition matrix}\\\\\n",
    "    Q(h_n)_{i,j} &= \\sigma^2 \\frac{h_n^{2q + 3 - i - j}}{(2q + 3 - i - j)(q + 1 - i)!(q + 1 - j)!} &\\text{Diffusion matrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76b226-f229-44ab-be00-d407427af899",
   "metadata": {},
   "source": [
    "What does this mean?\n",
    "\n",
    "Consider again the example from above:\n",
    "$$ x_n = \\begin{pmatrix} y_n \\\\ y'_n \\\\ y''_n \\\\ y'''_n \\end{pmatrix}\\,, $$\n",
    "then, with $h := h_{n+1}, A := A(h_{n+1})$ for less clutter:\n",
    "$$ (Ax_n)_1 =\n",
    "\\begin{bmatrix}\\frac{h^0}{0!} & \\frac{h^1}{1!} & \\frac{h^2}{2!} & \\frac{h^3}{3!}\\end{bmatrix}\n",
    "\\begin{pmatrix} y_n \\\\ y'_n \\\\ y''_n \\\\ y'''_n \\end{pmatrix}\n",
    "= y_n + h y'_n + \\frac{h^2}{2} y''_n + \\frac{h^3}{6} y'''_n\\,, $$\n",
    "which is a $q=3$-rd order Tayler expansion of $y(t_n + h_{n+1})$.\n",
    "\n",
    "- Thus, we'll also get a solver of order $q$ (and local error $\\mathcal{O}(h^{q+1})$)\n",
    "  - This is _just for illustration_, absolutely not a proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff35a0-e483-4071-bc0d-02b62d60df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_transition_matrix(d, q, h):\n",
    "    A_exponents = jnp.stack([jnp.arange(q + 1) - i for i in range(q + 1)])\n",
    "    A = jnp.triu(h ** A_exponents / jax.scipy.special.factorial(A_exponents))\n",
    "    return jnp.kron(A, jnp.eye(d))  # Ignore everything about d/this line for now\n",
    "\n",
    "def discrete_diffusion_matrix(d, q, h):\n",
    "    Q_j = jnp.stack((jnp.arange(q + 1) + 1,) * (q + 1))\n",
    "    Q_exponent = 2 * q + 3 - Q_j.T - Q_j\n",
    "    Q_divisor_j = jax.scipy.special.factorial(q + 1 - Q_j)\n",
    "    Q = (h ** Q_exponent  # Note the absence of the \\sigma^2 factor,\n",
    "         / (Q_exponent    # we'll come back to this\n",
    "             * Q_divisor_j\n",
    "             * Q_divisor_j.T))\n",
    "    return jnp.kron(Q, jnp.eye(d))  # Ignore everything about d/this line for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ab4a5-2761-41b7-8d1f-3352b0d6661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example with q=2 and h=0.1:\n",
    "A_ex = discrete_transition_matrix(1, 2, 0.1)\n",
    "Q_ex = discrete_diffusion_matrix(1, 2, 0.1)\n",
    "print(f\"A:\\n{A_ex}\")\n",
    "print(f\"Q:\\n{Q_ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8cb08-335b-43ba-b4be-6999e9f67593",
   "metadata": {},
   "source": [
    "#### Why use state misalignment z and not condition on $f(H_0 x_n)$ directly?\n",
    "\n",
    "- This separates the observations from the implementation/algorithm, ie. we observe the data $z_n = 0$ for _any_ implementation\n",
    "  - If we changed the observations to:\n",
    "    - $\\tilde{z}_n = Hx_n$\n",
    "    - and want to observe that $\\tilde{z}_n = f(H_0 x_n)$ directly\n",
    "  - then now our observation/data that we condition on depend on the algorithm\n",
    "    - as $x_n$, the current solution/state, depends on the algorithm in use\n",
    "    - Every algorithm would operate on different observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9540027-57ed-402c-be27-341f3b444bcc",
   "metadata": {},
   "source": [
    "## Filtering and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261b361-fb32-44f3-86e2-1a79026794ba",
   "metadata": {},
   "source": [
    "### Kalman (ODE) Filter\n",
    "\n",
    "For now, let's first consider the linear Gaussian SSM case from above:\n",
    "\\begin{align}\n",
    "    p(x_0) &= \\mathcal{N}(x_0; m_0, P_0) &\\text{Initial value}\\\\\n",
    "    p(x_{n+1} \\mid x_n) &= \\mathcal{N}(x_{n+1}; A(h_{n+1})x_n, Q(h_{n+1})) &\\text{Dynamics}\\\\\n",
    "    p(z_n \\mid x_n) &= \\mathcal{N}(z_n; g(x_n), R) &\\text{Observations (information operator)}\\end{align}\n",
    "\n",
    "On this, we can directly perform Bayesian inference.\n",
    "As the SSM fulfills the Markov property, we can perform filtering by iterating two steps:\n",
    "\n",
    "- Predict: $p(x_{n+1} | z_{0:n}) = \\int p(x_{n+1} | x_n)p(x_n | z_{0:n}) dx_n$ (Chapman-Kolmogorov Eq.)\n",
    "- Observe the next $z_{n+1}$\n",
    "  - Actually, observe that $z_{n+1} = 0$\n",
    "  - So we don't actually need to do anything\n",
    "- Update: $p(x_{n+1} | z_{0:n+1}) = \\frac{p(z_{n+1} | x_{n+1})p(x_{n+1} | z_{0:n})}{p(z_{n+1}|z_{0:n})}$\n",
    "\n",
    "As (for now) everything is linear Gaussian, the above just boils down\n",
    "to a little bit of linear algebra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe28074-6f4b-4c1f-b321-f88fa5999c78",
   "metadata": {},
   "source": [
    "#### Prediction Step\n",
    "\n",
    "$$ p(x_{n+1} | z_{0:n}) = \\mathcal{N}(x_{n+1}; m_{n+1}^p, P_{n+1}^p) $$\n",
    "where\n",
    "\\begin{align*}\n",
    "    m_{n+1}^p &= A(h_{n+1}) m_n^f &\\text{predictive mean}\\\\\n",
    "    P_{n+1}^p &= A(h_{n+1}) P_n^f A(h_{n+1})^\\top + Q(h_{n+1}) &\\text{predictive cov}\\,,\n",
    "\\end{align*}\n",
    "and this is simply implemented by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bd205-69f1-48f7-b2a1-6020b0a19c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(m_f, P_f, A, Q):\n",
    "    m_p = A @ m_f  # predictive mean\n",
    "    P_p = A @ P_f @ A.T + Q  # predictive cov\n",
    "    return m_p, P_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721e08a-54d8-4e61-8114-88865f9c3802",
   "metadata": {},
   "source": [
    "#### Exact Update Step ($f$ linear)\n",
    "\n",
    "$$ p(x_{n+1} | z_{0:n+1}) = \\mathcal{N}(x_{n+1}; m_{n+1}^f, P_{n+1}^f) $$\n",
    "where\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= H &\\text{(for now)} \\\\\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov} \\,.\n",
    "\\end{align*}\n",
    "\n",
    "As long as $f$ is linear, everything stays linear Gaussian.\n",
    "- This is known as the _Kalman filter_\n",
    "- And is exact\n",
    "\n",
    "Before we implement this: What can we do for nonlinear $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4ca82-73c8-4392-840e-242fd8d39f94",
   "metadata": {},
   "source": [
    "#### Extended Kalman ODE Filter (EKF0, EKF1)\n",
    "\n",
    "We need to stay linear Gaussian, but our $f$ now is nonlinear :(\n",
    "\n",
    "What to do now?\n",
    "- We can just approximate $f$ around $H_0 m_{n+1}^p$ by a linear function, then do the above\n",
    "- Thus, the update is now no longer exact, but:\n",
    "  - This is very cheap\n",
    "  - and often accurate enough\n",
    "\n",
    "This is known as Extended Kalman (ODE) Filter (EKF).\n",
    "\n",
    "We have two choices for our approximation of $f$:\n",
    "- constant function (order 0 Taylor expansion)\n",
    "  - EKF0\n",
    "  - $f(H_0 m_{n+1}^p + y) \\approx f(H_0 m_{n+1}^p)$\n",
    "- linear function (order 1 Taylor expansion)\n",
    "  - EKF1\n",
    "  - $f(H_0 m_{n+1}^p + y) \\approx f(H_0 m_{n+1}^p) + J_f(H_0 m_{n+1}^p)(y - H_0 m_{n+1}^p)$\n",
    "\n",
    "From this, we obtain two choices for the EKF update step:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= \\begin{cases}H &\\text{EKF0} \\\\ H - J_f(H_0 m_{n+1}^p) H_0 &\\text{EKF1}\\end{cases} &\\text{cov. of $z_{n+1}$ after approximation}\\\\\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov} \\,.\n",
    "\\end{align*}\n",
    "\n",
    "With that, we can directly implement the update:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b75e7-a608-4f9b-bb23-1d542d276d7a",
   "metadata": {},
   "source": [
    "Calculate:\n",
    "\\begin{align*}\n",
    "    \\hat{z}_{n+1} &= f(H_0 m_{n+1}^p) - Hm_{n+1}^p &\\text{innovation residual} \\\\\n",
    "    \\tilde{H} &= \\begin{cases}H &\\text{EKF0} \\\\ H - J_f(H_0 m_{n+1}^p) H_0 &\\text{EKF1}\\end{cases} &\\text{cov. of $z_{n+1}$ after approximation}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b8479-c425-435f-bb58-894906fbeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(f, t, m_p, H, H0, f_args, ekf_order):\n",
    "    # returns the innovation residual/state misalignment\n",
    "    m_p0 = H0 @ m_p\n",
    "    f_at_m_p = f(t, m_p0, f_args)\n",
    "    if ekf_order == 0:\n",
    "        H_hat = H\n",
    "    else:\n",
    "        # ekf_order == 1 (if invalid, just treat as 1)\n",
    "        # Calculate Jacobian of f with forward mode AD\n",
    "        J_f = jax.jacfwd(f, argnums=1)(t, m_p0, f_args)\n",
    "        H_hat = H - J_f @ H0  # regular * in 1-d case\n",
    "    z_hat = f_at_m_p - H @ m_p  # residual\n",
    "    return z_hat, H_hat  # H_hat: \\tilde{H} from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434af55b-ec08-44b0-b1ef-d0683b1b25fd",
   "metadata": {},
   "source": [
    "Calculate:\n",
    "\\begin{align*}\n",
    "    S_{n+1} &= \\tilde{H} P_{n+1}^p \\tilde{H}^\\top + R &\\text{innovation cov (intermediate)} \\\\\n",
    "    K_{n+1} &= P_{n+1}^f \\tilde{H}^\\top S_{n+1}^{-1} &\\text{Kalman gain (intermediate)} \\\\\n",
    "    m_{n+1}^f &= m_{n+1}^p + K_{n+1} \\hat{z}_{n+1}  &\\text{filtering mean (result)} \\\\\n",
    "    P_{n+1}^f &= (I_D - K_{n+1} \\tilde{H}) P_{n+1}^p &\\text{filtering cov (result)} \\,.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d91b6-c738-4337-8f75-2da392928d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R):\n",
    "    # Kalman filter statistics\n",
    "    S = H_hat @ P_p @ H_hat.T + R\n",
    "    S_inv = jnp.linalg.inv(S)  # Yes, inverse here. We'll improve this later.\n",
    "    K = P_p @ H_hat.T @ S_inv\n",
    "    # Filtering mean/cov\n",
    "    m_f = m_p + K @ z_hat\n",
    "    P_f = (jnp.eye(P_p.shape[1]) - K @ H_hat) @ P_p\n",
    "    return m_f, P_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69b5d3-f317-4f14-b50f-f39ad703ac73",
   "metadata": {},
   "source": [
    "#### Filter step\n",
    "\n",
    "Thus, we can build a full filter step (predict, then update) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0bae1-576e-49c7-b15c-4e6a0678ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jit compile this function for speedup\n",
    "@partial(jax.jit, static_argnames=[\"f\", \"f_args\", \"d\", \"q\", \"ekf_order\"])\n",
    "def ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                    t_prev, h, d, q,\n",
    "                    H0, H, R, ekf_order):\n",
    "    # calculate next time step\n",
    "    t = t_prev + h\n",
    "    # Get discretization for stepsize h:\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    Q = discrete_diffusion_matrix(d, q, h)\n",
    "    # predict\n",
    "    m_p, P_p = predict(m_f_prev, P_f_prev, A, Q)\n",
    "    # update\n",
    "    z_hat, H_hat = residual(f, t, m_p, H, H0, f_args, ekf_order)\n",
    "    m_f, P_f = next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R)\n",
    "    # Also extract and return the current (filtering) y and its stddev\n",
    "    y = H0 @ m_f\n",
    "    stddev = jnp.sqrt(jnp.diag(H0 @ P_f @ H0.T))\n",
    "    return m_f, P_f, m_p, P_p, y, stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc1bbd-7f7b-4912-9a18-00f62d285a0b",
   "metadata": {},
   "source": [
    "#### How to get the initial state vector?\n",
    "\n",
    "Remember, the state vector $x_n$ contains $y_n$ and $y'_n$,\n",
    "and, if we want, derivatives up to $y^{(q)}_n$.\n",
    "\n",
    "If $q = 1$, then it is obvious that $x_0 = \\begin{pmatrix} y_0 & f(y_0) \\end{pmatrix}^\\top$,\n",
    "but how do we handle higher $q$?\n",
    "- The elements of $x$ are, and stay, derivatives of each other, so we need to initialize\n",
    "  the first mean with higher-order derivatives as follows:\n",
    "  $$ m_0^f = \\begin{pmatrix} y_0 & f^{\\langle 1 \\rangle}(y_0) & f^{\\langle 2 \\rangle}(y_0) & \\ldots & f^{\\langle q \\rangle}(y_0)\\end{pmatrix}^\\top\\,, $$\n",
    "  where\n",
    "  \\begin{align*}\n",
    "    f^{\\langle 1 \\rangle}(y) &= f(y)\\,, \\\\\n",
    "    f^{\\langle i \\rangle}(y) &= \\frac{d}{dy} \\left( f^{\\langle i-1 \\rangle}(y) \\cdot f(y) \\right) & \\text{(for $i>1$)}\\,.\n",
    "  \\end{align*}\n",
    "  This is exponentially (in $q$) expensive with regular AD,\n",
    "  but can be more efficiently calculated using Taylor-mode AD, in JAX implemented in ```jet.jet()```.\n",
    "\n",
    "\n",
    "If interested, more details can be found in [Bettencourt, Johnson, Duvenaud (2019)](https://openreview.net/pdf?id=SkxEF3FNPH]).\n",
    "\n",
    "- This is the only thing we need to worry about (apart from stability issues, maybe addressed at the end)\n",
    "\n",
    "Here's an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e075e-e982-428b-b479-1bb73f4364e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state(f, t0, y0, q, f_args):\n",
    "    # Return [y0, f^<i>(y0)] as defined above\n",
    "    def f_wrapped(y):\n",
    "        # make f a function of y only (so it works with jet)\n",
    "        return f(t0, y, f_args)\n",
    "    y0_tup = (y0,)  # jet wants this in a tuple\n",
    "    # y_is will now iteratively be filled like this:\n",
    "    # (f(y0), f^<1>(y0), f^<2>(y0))  ->  (f(y0),) + (f^<1>(y0), f^<2>(y0), f^<3>(y0))\n",
    "    y_is = (f_wrapped(y0),)\n",
    "    for _ in range(q - 1):\n",
    "        y_i_new, y_is_new = jet.jet(f_wrapped, y0_tup, (y_is,))\n",
    "        # above returns tuple[Array, list[Array]], put all into one tuple\n",
    "        y_is = (y_i_new,) + (*y_is_new,)\n",
    "    return jnp.stack((y0_tup) + y_is).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c746d-0f3b-4268-8685-5f2b632607cf",
   "metadata": {},
   "source": [
    "#### How to fit higher-dimensional y? (ie. an ODE system)\n",
    "\n",
    "If $y$ is $d$-dimensional, then we can just stack everything, using the Kronecker product $\\otimes$,\n",
    "with\n",
    "$$\n",
    "    \\mathbf{A} = A \\otimes I_d\n",
    "       = \\begin{bmatrix}\n",
    "          a_{1,1}I_d & \\cdots & a_{1,q+1}I_d \\\\\n",
    "          \\vdots & \\ddots & \\vdots \\\\\n",
    "          a_{q+1,1}I_d & \\cdots & a_{q+1,q+1}I_d\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{d(q+1)\\times d(q+1)}\\,.\n",
    "$$\n",
    "For example, for $d=2, q=1$, we have\n",
    "$$ \\mathbf{x} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_1' \\\\ y_2' \\end{pmatrix}\\,, $$\n",
    "and $\\mathbf{A} = A \\otimes I_2$.\n",
    "\n",
    "The functions given above for $A, Q, H, H_0$ already do this, so let's see what this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5f677-c505-46cb-95ee-b1f7b875cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = jnp.array([0.0, 2.0])  # the first dimension\n",
    "x2 = jnp.array([1.0, 3.0])  # the second dimension\n",
    "x_s = jnp.stack([x1, x2]).T.flatten()\n",
    "A = discrete_transition_matrix(1, 1, 0.1)\n",
    "A_s = discrete_transition_matrix(2, 1, 0.1)\n",
    "print(f\"x1: {x1}, x2: {x2}\")\n",
    "print(f\"x_s: {x_s}\")\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"A_s:\\n{A_s}\")\n",
    "print(f\"Ax1: {A@x1}, Ax2: {A@x2}\")\n",
    "print(f\"Ax1, Ax2 stacked: {jnp.stack([A@x1, A@x2]).T.flatten()}\")\n",
    "print(f\"A_s x_s: {A_s@x_s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f26cfb-7454-4f9e-863b-a27d9cf8550d",
   "metadata": {},
   "source": [
    "Essentially, this means that all of the stacked operators\n",
    "just operate _per dimension_ as they would normally\n",
    "in the one-dimensional case.\n",
    "\n",
    "- This means that we don't need to care about $d$ anywhere,\n",
    "  as long as we stack the state and the operators correctly,\n",
    "  the math stays exactly the same.\n",
    "- To my understanding, this also means that every dimension\n",
    "  is modeled independently of each other\n",
    "  - For example, we don't get covariances between different dimensions,\n",
    "    or rather, it is zero\n",
    "  - In the prior, every dimension is a $q$-times IWP independently\n",
    "    of every other dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafca2c9-f82b-4b49-b66d-adbd070f33b9",
   "metadata": {},
   "source": [
    "#### ODE Filter\n",
    "\n",
    "Now, we can build our first probabilistic ODE solver\n",
    "- with constant stepsize $h$\n",
    "- no error estimation etc.\n",
    "- lots of other points we can adress later\n",
    "a simple version of an ODE filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1726f3-16b3-425f-bdf7-65314aacefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_filter(y0, f, f_args,\n",
    "               t0, t1,\n",
    "               q,  # order\n",
    "               h,\n",
    "               ekf_order=1):\n",
    "    h, t0 = jnp.array(h), jnp.array(t0)  # Ensure these are arrays\n",
    "    d = y0.shape[-1]  # dimension of y\n",
    "    # Initialization\n",
    "    x0 = initial_state(f, t0, y0, q, f_args)\n",
    "    stddev0 = jnp.zeros_like(y0)\n",
    "    P0 = jnp.diag(jnp.repeat(stddev0, q + 1))  # Assume zero initial cov.\n",
    "    H0, H = ssm_projection_matrices(d, q)\n",
    "    # R=0 -> dirac measure on initial value\n",
    "    R = jnp.zeros((d, d))\n",
    "    # Init. objects for storing results\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    ts, ys, stddevs = [t0], [y0], [stddev0]\n",
    "    filtering_means, filtering_covs, predictive_means, predictive_covs = [x0], [P0], [], []\n",
    "    # Init. loop variables\n",
    "    t_prev = t0\n",
    "    m_f_prev, P_f_prev = x0, P0  # Filtering parameters (of previous step)\n",
    "\n",
    "    # The ODE filter loop\n",
    "    while t_prev < t1:\n",
    "        m_f, P_f, m_p, P_p, y, stddev = \\\n",
    "            ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                            t_prev, h, d, q,\n",
    "                            H0, H, R, ekf_order)\n",
    "        # Store results\n",
    "        t = t_prev + h\n",
    "        ts.append(t)\n",
    "        ys.append(y)\n",
    "        stddevs.append(stddev)\n",
    "        filtering_means.append(m_f)\n",
    "        filtering_covs.append(P_f)\n",
    "        predictive_means.append(m_p)\n",
    "        predictive_covs.append(P_p)\n",
    "        # Update loop variables\n",
    "        t_prev, m_f_prev, P_f_prev = t, m_f, P_f\n",
    "\n",
    "    # Return results in dict\n",
    "    return {\n",
    "        \"ts\": jnp.asarray(ts).squeeze(),\n",
    "        \"ys\": jnp.asarray(ys).squeeze(),\n",
    "        \"stddevs\": jnp.asarray(stddevs).squeeze(),\n",
    "        \"predictive_means\": predictive_means,\n",
    "        \"predictive_covs\": predictive_covs,\n",
    "        \"filtering_means\": filtering_means,\n",
    "        \"filtering_covs\": filtering_covs,\n",
    "        \"H0\": H0, \"d\": d, \"q\": q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cded317-c72b-41a4-a5cb-0efcb67d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 1\n",
    "h = 1e-1  # If we decrease the stepsize, this is very accurate already\n",
    "#h = 1e-2\n",
    "ekf_order = 1\n",
    "f_sol = ode_filter(\n",
    "    y0, lotka_volterra_vf, lv_args,\n",
    "    t0, t1, q, h, ekf_order=ekf_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8e614-2659-4d58-b651-e5a20db0cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lv_with_uncertainty(prob_sol, num_stddevs=2, c_sol=None, title=None):\n",
    "    predator_col = \"red\"\n",
    "    prey_col = \"green\"\n",
    "    if c_sol is not None:\n",
    "        plt.plot(sol.ts, sol.ys, label=None, linewidth=1, color=\"grey\")\n",
    "    plt.plot(prob_sol[\"ts\"], prob_sol[\"ys\"][:, 0], label=\"Prey\",\n",
    "             color=prey_col, linewidth=1)\n",
    "    plt.plot(prob_sol[\"ts\"], prob_sol[\"ys\"][:, 1], label=\"Predator\",\n",
    "             color=predator_col, linewidth=1)\n",
    "    plt.fill_between(prob_sol[\"ts\"],\n",
    "                     prob_sol[\"ys\"][:, 0] - num_stddevs*prob_sol[\"stddevs\"][:, 0],\n",
    "                     prob_sol[\"ys\"][:, 0] + num_stddevs*prob_sol[\"stddevs\"][:, 0],\n",
    "                     label=f\"Prey $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=prey_col, alpha=0.4)\n",
    "    plt.fill_between(prob_sol[\"ts\"],\n",
    "                     prob_sol[\"ys\"][:, 1] - num_stddevs*prob_sol[\"stddevs\"][:, 1],\n",
    "                     prob_sol[\"ys\"][:, 1] + num_stddevs*prob_sol[\"stddevs\"][:, 1],\n",
    "                     label=f\"Predator $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=predator_col, alpha=0.4)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend(prop={'size': 6})\n",
    "    plt.show()\n",
    "\n",
    "def plot_filter_and_smoother(f_out, s_out, num_stddevs=2, c_sol=None, title=None):\n",
    "    predator_col = \"red\"\n",
    "    prey_col = \"green\"\n",
    "    filter_col = \"grey\"\n",
    "    if c_sol is not None:\n",
    "        plt.plot(sol.ts, sol.ys, label=None, linewidth=1, color=\"black\")\n",
    "    plt.plot(f_out[\"ts\"], f_out[\"ys\"][:, 0], label=\"Prey (Filter)\",\n",
    "             color=filter_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(f_out[\"ts\"], f_out[\"ys\"][:, 1], label=\"Predator (Filter)\",\n",
    "             color=filter_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(s_out[\"ts\"], s_out[\"ys\"][:, 0], label=\"Prey (Smoother)\",\n",
    "             color=prey_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.plot(s_out[\"ts\"], s_out[\"ys\"][:, 1], label=\"Predator (Smoother)\",\n",
    "             color=predator_col, linewidth=1, linestyle=\"-\")\n",
    "    plt.fill_between(f_out[\"ts\"],\n",
    "                     f_out[\"ys\"][:, 0] - num_stddevs*f_out[\"stddevs\"][:, 0],\n",
    "                     f_out[\"ys\"][:, 0] + num_stddevs*f_out[\"stddevs\"][:, 0],\n",
    "                     #label=f\"Prey (Filter) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     label=None,\n",
    "                     linewidth=0, color=filter_col, alpha=0.3)\n",
    "    plt.fill_between(f_out[\"ts\"],\n",
    "                     f_out[\"ys\"][:, 1] - num_stddevs*f_out[\"stddevs\"][:, 1],\n",
    "                     f_out[\"ys\"][:, 1] + num_stddevs*f_out[\"stddevs\"][:, 1],\n",
    "                     #label=f\"Predator (Filter) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     label=None,\n",
    "                     linewidth=0, color=filter_col, alpha=0.3)\n",
    "    plt.fill_between(s_out[\"ts\"],\n",
    "                     s_out[\"ys\"][:, 0] - num_stddevs*s_out[\"stddevs\"][:, 0],\n",
    "                     s_out[\"ys\"][:, 0] + num_stddevs*s_out[\"stddevs\"][:, 0],\n",
    "                     label=f\"Prey (Smoother) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=prey_col, alpha=0.4)\n",
    "    plt.fill_between(s_out[\"ts\"],\n",
    "                     s_out[\"ys\"][:, 1] - num_stddevs*s_out[\"stddevs\"][:, 1],\n",
    "                     s_out[\"ys\"][:, 1] + num_stddevs*s_out[\"stddevs\"][:, 1],\n",
    "                     label=f\"Predator (Smoother) $\\\\pm {num_stddevs}\\\\sigma$\",\n",
    "                     linewidth=0, color=predator_col, alpha=0.4)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.legend(prop={'size': 6})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e5226-4cfa-498e-b1eb-c4c6ea621716",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lv_with_uncertainty(f_sol, c_sol=sol, title=f\"ODE Filter, q={q}, h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41fd74-38ac-47e6-b983-7059258d4a6e",
   "metadata": {},
   "source": [
    "### Smoother\n",
    "\n",
    "Great, our filter seems to be working decently!\n",
    "\n",
    "But, it only calculates the filtering distribution $p(x_n | z_{1:n})$.\n",
    "\n",
    "As we can see above, this is pretty useful already.\n",
    "However, at every $x_n$, it only considers information up to that time point.\n",
    "\n",
    "We can also perform a smoothing pass, where we go backwards in time\n",
    "to obtain the full smoothing posterior\n",
    "$$ p(x_n | z_{1:N}) = \\mathcal{N}(x_n; m_n^S, P_n^S) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6a86b-e329-417f-bb89-427869616581",
   "metadata": {},
   "source": [
    "#### Smoother step\n",
    "\n",
    "We start with $m_N^S = m_N^f, P_N^S = P_N^f$ (ie. set the last smoothing parameters to the last filtering parameters), and then iteratively perform the step\n",
    "\n",
    "\\begin{align*}\n",
    "    G_n &= P_n^f A(h_{n+1})^\\top \\left(P_{n+1}^p\\right)^{-1} &\\text{Gain} \\\\\n",
    "    m_n^S &= m_n^f + G_n(m_{n+1}^S - m_{n+1}^p) &\\text{smoothing mean} \\\\\n",
    "    P_n^S &= P_n^f + G_n(P_{n+1}^S - P_{n+1}^p) G_n^\\top &\\text{smoothing cov}\\,,\n",
    "\\end{align*}\n",
    "\n",
    "which we can easily implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127f39a-336d-4fdd-924e-453c68093177",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"d\", \"q\"])\n",
    "def smoother_step(\n",
    "        m_f, P_f,\n",
    "        m_p_next, P_p_next,\n",
    "        m_s_next, P_s_next,\n",
    "        H0, h, d, q):\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    G = P_f @ A.T @ jnp.linalg.inv(P_p_next)  # again, we'll improve this later.\n",
    "    m_s = m_f + G @ (m_s_next - m_p_next)  # posterior smoothing mean\n",
    "    P_s = P_f + G @ (P_s_next - P_p_next) @ G.T  # posterior smoothing cov\n",
    "    y_s = H0 @ m_s  # Again extract current y and stddev\n",
    "    stddev_s = jnp.sqrt(jnp.diag(H0 @ P_s @ H0.T))\n",
    "    return m_s, P_s, y_s, stddev_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88666c1b-a07a-49ef-bc3c-8d62b8ddd6aa",
   "metadata": {},
   "source": [
    "And can already implement the our ODE smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8ce40-b36b-42a1-aa85-d3c1081d0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_smoother(f_out):\n",
    "    # Setup, retrieve parameters\n",
    "    predictive_means = f_out[\"predictive_means\"]\n",
    "    predictive_covs = f_out[\"predictive_covs\"]\n",
    "    filtering_means = f_out[\"filtering_means\"]\n",
    "    filtering_covs = f_out[\"filtering_covs\"]\n",
    "    ts = f_out[\"ts\"]\n",
    "    N, d, q = len(ts), f_out[\"d\"], f_out[\"q\"]\n",
    "    H0 = f_out[\"H0\"]\n",
    "    # Init. loop variables\n",
    "    m_s_next = filtering_means[-1]\n",
    "    P_s_next = filtering_covs[-1]\n",
    "    smoothing_ts = [ts[-1]]  # Init. last smoothing parameters as last filtering parameters\n",
    "    smoothing_ys = [f_out[\"ys\"][-1]]\n",
    "    smoothing_stddevs = [f_out[\"stddevs\"][-1]]\n",
    "\n",
    "    for n in range(N - 2, -1, -1):  # shift by 1 due to zero-based indexing\n",
    "        # [N - 2, 0]\n",
    "        # (N-1 is last, and that step was already initialized as the\n",
    "        # last filtering parameters).\n",
    "        # Predictive params hold one less element at the start,\n",
    "        # so eg predictive_means[n] actually corresponds to m_p[n+1]\n",
    "        m_p_next, P_p_next = predictive_means[n], predictive_covs[n]\n",
    "        m_f, P_f = filtering_means[n], filtering_covs[n]\n",
    "        t = ts[n]\n",
    "        h = ts[n + 1] - t\n",
    "        m_s, P_s, y, stddev = smoother_step(\n",
    "            m_f, P_f, m_p_next, P_p_next, m_s_next, P_s_next, H0, h, d, q)\n",
    "        # Store results\n",
    "        smoothing_ts.insert(0, t)\n",
    "        smoothing_ys.insert(0, y)\n",
    "        smoothing_stddevs.insert(0, stddev)\n",
    "        m_s_next, P_s_next = m_s, P_s\n",
    "\n",
    "    return {\n",
    "            \"ts\": jnp.asarray(smoothing_ts).squeeze(),\n",
    "            \"ys\": jnp.asarray(smoothing_ys).squeeze(),\n",
    "            \"stddevs\": jnp.asarray(smoothing_stddevs).squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffaf4d-0064-47f1-902d-260bd99e30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_sol = ode_smoother(f_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfca2e2-8957-4061-9f27-eea9f667490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_lv_with_uncertainty(s_sol, c_sol=sol, title=f\"ODE Smoother\")\n",
    "plot_filter_and_smoother(f_sol, s_sol, c_sol=sol, title=f\"ODE Filter and Smoother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9dfb5-89fb-4ab4-b929-b48804331ded",
   "metadata": {},
   "source": [
    "## Extending the simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33fa19-a131-4c1b-842e-69febcb067ce",
   "metadata": {},
   "source": [
    "### Uncertainty estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684c151-be78-41e5-85dd-c9ed3bc061a4",
   "metadata": {},
   "source": [
    "#### Calibrated Local Uncertainty\n",
    "\n",
    "Remember the IWP scale $\\sigma$ in [Dynamic model / State](#Dynamic-model-/-State)?\n",
    "\n",
    "So far, we've just used $\\sigma = 1$.\n",
    "\n",
    "We can get a _local_ (quasi-ML) estimate $\\hat{\\sigma}_n^2$ via\n",
    "$$ \\hat{\\sigma}_n^2 = \\hat{z}_n^\\top \\left(H \\tilde{Q}(h_n) H^\\top \\right)^{-1} \\hat{z}_n^\\top\\,, $$\n",
    "where $\\tilde{Q}(h_n)$ is $Q(h_n)$ with $\\sigma^2 = 1$\n",
    "(as, remember, $Q(h_n)$ does depend on $\\sigma^2$).\n",
    "\n",
    "This is actually what the function ```discrete_diffusion_matrix(d, q, h)``` does\n",
    "(and the reason why we ignored the factor $\\sigma^2$).\n",
    "\n",
    "So, for calibrated (local) uncertainty, we can just run the filter like before,\n",
    "and, after we've calculated $\\hat{z}_n$, we can get our uncertainty estimate $\\hat{\\sigma}_n^2$,\n",
    "and just rescale $Q(h_n) = \\hat{sigma}_n^2 \\tilde{Q}(h_n)$, and have calibrated local uncertainty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9563ebd-28b9-4176-a76e-24f87d0d80d4",
   "metadata": {},
   "source": [
    "#### Local Error Estimation\n",
    "\n",
    "Additionally, we can now use our calibrated $Q(h_n) = \\hat{\\sigma}_n^2 \\tilde{Q}(h_n)$\n",
    "to obtain an estimate for the local error with\n",
    "$$ D(h_n) = \\sqrt{\\tilde{H} Q(h_n) \\tilde{H}^\\top}\\,, $$\n",
    "which is essentially just the covariance of $\\hat{z}_n$.\n",
    "\n",
    "If we have a multi-dimensional ODE, then the above estimates an error per dimension.\n",
    "\n",
    "I just took the max error over the dimension as estimate\n",
    "- Although I'm not sure this is the best way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f6e94-4d9f-4781-bf5a-a16f1bf434ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"f\", \"f_args\", \"d\", \"q\", \"ekf_order\"])\n",
    "def ode_filter_step(m_f_prev, P_f_prev, f, f_args,\n",
    "                    t_prev, h, d, q,\n",
    "                    H0, H, R, ekf_order):\n",
    "    t = t_prev + h\n",
    "    A = discrete_transition_matrix(d, q, h)\n",
    "    Q = discrete_diffusion_matrix(d, q, h)\n",
    "    # predict\n",
    "    m_p, _ = predict(m_f_prev, P_f_prev, A, Q)  # Don't need P_p now\n",
    "    # residual\n",
    "    z_hat, H_hat = residual(f, t, m_p, H, H0, f_args, ekf_order)\n",
    "    # local uncertainty est.\n",
    "    sigma_sq = z_hat.T @ jnp.linalg.inv(H @ Q @ H.T)^{-1} \\ z_hat\n",
    "    Q = sigma_sq * Q  # update Q\n",
    "    local_error = jnp.max(jnp.sqrt(H_hat @ Q @ H_hat.T))  # est. local error\n",
    "    _, P_p = predict(m_f_prev, P_f_prev, A, Q)  # predict again (P_p depends on Q)\n",
    "\n",
    "    m_f, P_f = next_filtering_mean_and_cov(m_p, P_p, z_hat, H_hat, R)\n",
    "\n",
    "    y = H0 @ m_f\n",
    "    stddev = jnp.sqrt(jnp.diag(H0 @ P_f @ H0.T))\n",
    "    return m_f, P_f, m_p, P_p, y, stddev, local_error  # report the local error est."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95104f32-8398-47b9-925a-0a700528145d",
   "metadata": {},
   "source": [
    "#### Step size adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf5c2f-8114-4438-8622-9626316ca9d5",
   "metadata": {},
   "source": [
    "### Improving numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d259c-158d-4f76-a7b9-27a035b8a2d6",
   "metadata": {},
   "source": [
    "#### Nordsieck-like Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d071e-9e7d-41ba-9e01-9b02fd669eef",
   "metadata": {},
   "source": [
    "#### Square-Root Kalman Filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
